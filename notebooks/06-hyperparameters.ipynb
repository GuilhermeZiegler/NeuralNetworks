{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTAÇÕES NECESSÁRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../scripts'))\n",
    "from myFunctions import install_packages\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import os\n",
    "\n",
    "def install_packages():\n",
    "    required_packages = [\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"joblib\",\n",
    "        \"pyarrow\",\n",
    "        \"fastparquet\",\n",
    "        \"plotly\",\n",
    "        \"matplotlib\",\n",
    "        \"MetaTrader5\",\n",
    "        \"tabulate\",\n",
    "        \"optuna\",\n",
    "        \"torch\",\n",
    "        \"tqdm\",\n",
    "        \"shap\",\n",
    "        \"kaleido\"\n",
    "    ]\n",
    "    \n",
    "    print(f'Installing required packages: {required_packages}')\n",
    "    # Checking installed packages\n",
    "    installed_packages = {pkg.key for pkg in pkg_resources.working_set}\n",
    "    \n",
    "    # Install missing packages\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            if package.lower() not in installed_packages:\n",
    "                print(f\"Installing {package}...\")\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            else:\n",
    "                print(f\"{package} is already installed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error installing {package}: {e}\")\n",
    "            continue  # Continue with other packages, log the error but don't stop the process\n",
    "    \n",
    "    print(\"All packages are verified.\")\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing required packages\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import kaleido\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL CLASSES DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based model for time-series tasks, supporting regression and classification.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features per time step.\n",
    "        hidden_size (int): Number of hidden units in each LSTM layer.\n",
    "        num_layers (int): Number of LSTM layers.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, target):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size // 2, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-based model for time-series tasks, supporting regression and classification.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features per time step.\n",
    "        hidden_size (int): Number of hidden units in each GRU layer.\n",
    "        num_layers (int): Number of GRU layers.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, target):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size // 2, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hidden = self.gru(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid CNN-LSTM model for time-series tasks, supporting regression and classification.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features per time step.\n",
    "        hidden_size (int): Number of hidden units in each LSTM layer.\n",
    "        num_layers (int): Number of LSTM layers.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "        conv_filters (int): Number of filters in the 1D convolutional layer.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, conv_filters, target):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_size, out_channels=conv_filters, kernel_size=3, stride=1, padding=1)\n",
    "        self.lstm = nn.LSTM(conv_filters, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size // 2, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output\n",
    "\n",
    "class CNNGRUModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid CNN-GRU model for time-series tasks, supporting regression and classification.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features per time step.\n",
    "        hidden_size (int): Number of hidden units in each GRU layer.\n",
    "        num_layers (int): Number of GRU layers.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "        conv_filters (int): Number of filters in the 1D convolutional layer.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, conv_filters, target):\n",
    "        super(CNNGRUModel, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_size, out_channels=conv_filters, kernel_size=3, stride=1, padding=1)\n",
    "        self.gru = nn.GRU(conv_filters, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size // 2, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, hidden = self.gru(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESSING DATA TO ARRAYS FOR WINDOWS SIZE AND LOOK FORWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df, exclude_columns=['date', 'day']):\n",
    "    \"\"\"\n",
    "    Escalona todas as colunas do DataFrame, exceto as especificadas em exclude_columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): O DataFrame contendo os dados.\n",
    "        exclude_columns (list): Lista de colunas que não serão escalonadas.\n",
    "\n",
    "    Returns:\n",
    "        df_scaled (pd.DataFrame): DataFrame com as colunas escalonadas.\n",
    "        scalers (dict): Dicionário contendo os escaladores para cada coluna escalonada.\n",
    "    \"\"\"\n",
    "    scalers = {}\n",
    "    columns_to_scale = [col for col in df.columns if col not in exclude_columns]\n",
    "\n",
    "    df_scaled = df.copy()\n",
    "    for col in columns_to_scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_scaled[col] = scaler.fit_transform(df[[col]])\n",
    "        scalers[col] = scaler\n",
    "\n",
    "    return df_scaled, scalers\n",
    "\n",
    "\n",
    "def data_to_array(df, window_size, target, features):\n",
    "    \"\"\"\n",
    "    Prepares X and y with targets shifted for the next day after the window.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe containing the data.\n",
    "        window_size (int): The window size (e.g., 7 days).\n",
    "        target (str): The target column name (e.g., 'close_price_target').\n",
    "        features (list): List of feature column names (e.g., ['open', 'high', 'low', 'close']).\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Input features.\n",
    "        y (np.ndarray): Target values.\n",
    "        y_dates (np.ndarray): Dates associated with the targets.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    y_dates = []\n",
    "\n",
    "    for i in range(len(df) - window_size):\n",
    "        # Access the target column directly by its name\n",
    "        target_value = df.iloc[i + window_size][target]\n",
    "        y.append(target_value)\n",
    "        y_dates.append(df.iloc[i + window_size]['date'])\n",
    "\n",
    "        # Prepare the features using the provided column names\n",
    "        X.append(df.iloc[i:i + window_size][features].values)\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(y_dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLITING DATA TO TRAIN AND TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def segment_data(df, test_size=0.15):\n",
    "    \"\"\"\n",
    "    Segments the data into training and test sets based on the test size percentage.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be segmented.\n",
    "        test_size (float): The percentage of data to be used for testing.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Training data.\n",
    "        pd.DataFrame: Testing data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculates the size of the test set\n",
    "    test_len = int(len(df) * test_size)\n",
    "\n",
    "    # Segments the data\n",
    "    train_data = df[:-test_len]  # 85% for training\n",
    "    test_data = df[-test_len:]   # 15% for testing\n",
    "\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(trial, X_train, y_train, X_test, y_test, target, window_size, look_forward, model_type, study_name, model_dir):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model using the given parameters and data.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): The current optuna trial to optimize hyperparameters.\n",
    "        X_train (ndarray): The training input data (features).\n",
    "        y_train (ndarray): The target labels for the training data.\n",
    "        X_test (ndarray): The test input data (features).\n",
    "        y_test (ndarray): The target labels for the test data.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "        window_size (int): The size of the sliding window for time-series data.\n",
    "        look_forward (int): The number of steps ahead to predict.\n",
    "        model_type (str): The type of model ('LSTM', 'GRU', 'CNN-LSTM', 'CNN-GRU').\n",
    "        study_name (str): Name of the optuna study for logging purposes.\n",
    "        model_dir (str): Directory to save the model and trial results.\n",
    "\n",
    "    Returns:\n",
    "        float: The evaluation error (either MSE or AUC depending on the task).\n",
    "    \"\"\"\n",
    "    # Set up the device for computation (GPU if available, otherwise CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Convert input data to PyTorch tensors and move them to the appropriate device\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long if 'behavior' in target else torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long if 'behavior' in target else torch.float32).to(device)\n",
    "\n",
    "    # Suggest hyperparameters using Optuna\n",
    "    epochs = trial.suggest_int('epochs',350, 1000)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 3)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 128)\n",
    "\n",
    "    if model_type in ['CNN-LSTM', 'CNN-GRU']:\n",
    "        conv_filters = trial.suggest_categorical('conv_filters', [32, 64, 128])\n",
    "\n",
    "    # Initialize the model based on the selected model type\n",
    "    if model_type == 'LSTM':\n",
    "        model = LSTMModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, target=target).to(device)\n",
    "    elif model_type == 'GRU':\n",
    "        model = GRUModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, target=target).to(device)\n",
    "    elif model_type == 'CNN-LSTM':\n",
    "        model = CNNLSTMModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, conv_filters=conv_filters, target=target).to(device)\n",
    "    elif model_type == 'CNN-GRU':\n",
    "        model = CNNGRUModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, conv_filters=conv_filters, target=target).to(device)\n",
    "\n",
    "    # Define the optimizer and the loss function (CrossEntropyLoss for classification, MSELoss for regression)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss() if 'behavior' in target else nn.MSELoss()\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize a dictionary to track loss at each epoch\n",
    "    average_losses = {epoch: [] for epoch in range(epochs)}\n",
    "\n",
    "    # Training loop with dimension adjustments\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "\n",
    "            if 'behavior' in target:\n",
    "               output = output.view(-1, 2)\n",
    "            else:\n",
    "               output = output.view(-1)\n",
    "\n",
    "            y_batch = y_batch.view(-1)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        average_losses[epoch].append(avg_loss)\n",
    "\n",
    "        # Report the loss to Optuna for pruning and optimization\n",
    "        trial.report(avg_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Print the loss every 50 epochs\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'Epoch {epoch} loss: {avg_loss}')\n",
    "\n",
    "    # Create a directory to save trial results\n",
    "    trials_dir = os.path.join(model_dir, 'trials')\n",
    "    if not os.path.exists(trials_dir):\n",
    "        os.makedirs(trials_dir)\n",
    "\n",
    "    # Optionally, plot and save the loss decay curve (this part is commented out for now)\n",
    "    # plot_loss_decay(average_losses, epochs, study_name, trials_dir, trial.number)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for X_batch, y_batch in test_loader:\n",
    "          output = model(X_batch)\n",
    "\n",
    "          if 'behavior' in target:\n",
    "              _, predicted = torch.max(output, 1)\n",
    "\n",
    "          else:\n",
    "              predicted = output.squeeze()\n",
    "              predicted = predicted.reshape(-1)\n",
    "              print(len(predicted))\n",
    "\n",
    "          predictions.append(predicted.cpu().numpy())\n",
    "          true_values.append(y_batch.cpu().numpy())\n",
    "\n",
    "    if any(p.size == 0 for p in predictions) or any(t.size == 0 for t in true_values):\n",
    "      print(f\"Error: One or more arrays in 'predictions' or 'true_values' are empty. Pruning trial {trial.number}.\")\n",
    "      raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_values = np.concatenate(true_values, axis=0)\n",
    "\n",
    "    # Calculate error (Mean Squared Error for regression or AUC for classification)\n",
    "    if 'price' in target:\n",
    "        error = mean_squared_error(true_values, predictions)\n",
    "    else:\n",
    "        error = roc_auc_score(y_test.cpu().numpy(), predictions)\n",
    "\n",
    "    # Save loss decay data to a file\n",
    "    loss_decay_file = os.path.join(trials_dir, f\"{study_name}_trial_{trial.number}_loss_decay.pkl\")\n",
    "    with open(loss_decay_file, 'wb') as f:\n",
    "        joblib.dump(average_losses, f)\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_models(df: pd.DataFrame,\n",
    "                    targets: list,\n",
    "                    features: list,\n",
    "                    look_backs: list,\n",
    "                    look_forwards: list,\n",
    "                    target_dir: str = None,\n",
    "                    models: list = None,\n",
    "                    max_samples: int = 100):\n",
    "    study_results = {}\n",
    "\n",
    "    if target_dir is not None:\n",
    "        input_dir = os.path.join('..', 'data', 'models', target_dir)\n",
    "    else:\n",
    "        input_dir = os.path.join('..', 'data', 'models')\n",
    "\n",
    "    if not os.path.exists(input_dir):\n",
    "        os.makedirs(input_dir)\n",
    "\n",
    "    exclude_columns = ['date', 'day']\n",
    "    df_scaled, _ = scale_features(df, exclude_columns)\n",
    "    train_data, test_data = segment_data(df_scaled, test_size=0.15)\n",
    "\n",
    "    for window_size in look_backs:\n",
    "        for look_forward in look_forwards:\n",
    "            for target in targets:\n",
    "                X_train, y_train, y_dates = data_to_array(train_data.copy(),\n",
    "                                                          window_size,\n",
    "                                                          target,\n",
    "                                                          features)\n",
    "\n",
    "                X_test, y_test, _ = data_to_array(test_data.copy(),\n",
    "                                                  window_size,\n",
    "                                                  target,\n",
    "                                                  features)\n",
    "\n",
    "                if models is None:\n",
    "                    models = ['LSTM', 'GRU', 'CNN-LSTM', 'CNN-GRU']\n",
    "\n",
    "                for model_type in tqdm(models):\n",
    "                    study_name = f\"{model_type}_look_back_{window_size}_look_forward_{look_forward}_{target}\"\n",
    "                    model_dir = os.path.join(input_dir, study_name)\n",
    "\n",
    "                    if not os.path.exists(model_dir):\n",
    "                        os.makedirs(model_dir)\n",
    "\n",
    "                    # Define optimization direction\n",
    "                    direction = 'minimize' if 'price' in target else 'maximize'\n",
    "\n",
    "                    # Creating the Optuna study with a pruner\n",
    "\n",
    "                    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=350, interval_steps=1)\n",
    "                    study = optuna.create_study(direction=direction, study_name=study_name, pruner=pruner)\n",
    "                    study.optimize(\n",
    "                        lambda trial: train_evaluate_model(\n",
    "                            trial, X_train, y_train, X_test, y_test, target, window_size, look_forward, model_type, study_name, model_dir\n",
    "                        ),\n",
    "                        n_trials=max_samples\n",
    "                    )\n",
    "\n",
    "                    study_file = os.path.join(model_dir, f\"{study_name}_study.pkl\")\n",
    "                    joblib.dump(study, study_file)\n",
    "                    print(f\"{study_name} saved to {study_file}\")\n",
    "\n",
    "                    best_params = study.best_params\n",
    "                    best_trial_index = study.best_trial.number\n",
    "                    best_trial_value = study.best_value\n",
    "\n",
    "                    best_params_dict = {\n",
    "                        'best_params': best_params,\n",
    "                        'best_trial_index': best_trial_index,\n",
    "                        'best_trial_value': best_trial_value\n",
    "                    }\n",
    "                    params_file = os.path.join(model_dir, f\"{study_name}_best_params.pkl\")\n",
    "                    with open(params_file, \"wb\") as f:\n",
    "                        joblib.dump(best_params_dict, f)\n",
    "\n",
    "                    try:\n",
    "                        fig_optimization = vis.plot_optimization_history(study)\n",
    "                        fig_optimization.write_image(os.path.join(model_dir, f\"{study_name}_optimization_history.png\"))\n",
    "\n",
    "                        fig_importances = vis.plot_param_importances(study)\n",
    "                        fig_importances.write_image(os.path.join(model_dir, f\"{study_name}_param_importances.png\"))\n",
    "\n",
    "                        fig_slice = vis.plot_slice(study)\n",
    "                        fig_slice.write_image(os.path.join(model_dir, f\"{study_name}_slice_plot.png\"))\n",
    "                        print(f\"Images saved to {model_dir}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in plotting images for {study_name}: {e}\")\n",
    "\n",
    "                    print(f\"Saved to {model_dir}:\")\n",
    "                    print(f\"Best hyperparameters for {model_type}, lookback: {window_size}, look forward: {look_forward}, target {target}: {best_params}\")\n",
    "                    print(f\"Best trial index: {best_trial_index}, Best trial value: {best_trial_value}\")\n",
    "\n",
    "                    study_results[study_name] = {\n",
    "                        \"study\": study,\n",
    "                        \"best_params\": best_params,\n",
    "                        \"best_trial_index\": best_trial_index,\n",
    "                        \"best_trial_value\": best_trial_value,\n",
    "                        \"directory\": model_dir\n",
    "                    }\n",
    "\n",
    "    return study_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = pd.read_parquet(os.path.join('..', input_dir, 'df_daily.parquet')).replace(\"/\", \"\\\\\")\n",
    "df2 = pd.read_parquet(os.path.join('..', input_dir, 'df_timestamp.parquet')).replace(\"/\", \"\\\\\")\n",
    "daily_features = joblib.load(os.path.join(input_dir_features, 'daily_features.pkl'))\n",
    "timnestamp_features = joblib.load(os.path.join(input_dir_features, '15min_timestamp_features.pkl'))\n",
    "features = daily_features\n",
    "models =  ['LSTM']\n",
    "#, 'open_price_target', 'close_price_target']\n",
    "\n",
    "targets=['close_price_target']\n",
    "# targets = ['open_price_target', 'close_price_target']\n",
    "sliding_windows = [60]\n",
    "look_forwards = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the optimize_models function\n",
    "study_results = optimize_models(df=df_daily,\n",
    "                                targets=targets,\n",
    "                                features=features,\n",
    "                                look_backs=sliding_windows,\n",
    "                                look_forwards=look_forwards,\n",
    "                                models=models,\n",
    "                                target_dir='daily',\n",
    "                                max_samples=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
