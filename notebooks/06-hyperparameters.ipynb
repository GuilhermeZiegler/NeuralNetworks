{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTAÇÕES NECESSÁRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages: ['numpy', 'pandas', 'scikit-learn', 'joblib', 'pyarrow', 'fastparquet', 'plotly', 'matplotlib', 'seaborn', 'MetaTrader5', 'tabulate', 'optuna', 'torch', 'tqdm', 'shap', 'kaleido', 'statsmodels', 're']\n",
      "numpy is already installed.\n",
      "pandas is already installed.\n",
      "scikit-learn is already installed.\n",
      "joblib is already installed.\n",
      "pyarrow is already installed.\n",
      "fastparquet is already installed.\n",
      "plotly is already installed.\n",
      "matplotlib is already installed.\n",
      "seaborn is already installed.\n",
      "MetaTrader5 is already installed.\n",
      "tabulate is already installed.\n",
      "optuna is already installed.\n",
      "torch is already installed.\n",
      "tqdm is already installed.\n",
      "shap is already installed.\n",
      "kaleido is already installed.\n",
      "statsmodels is already installed.\n",
      "Installing re...\n",
      "Error installing re: Command '['c:\\\\Users\\\\guitz\\\\anaconda3\\\\envs\\\\pytorch_env\\\\python.exe', '-m', 'pip', 'install', 're']' returned non-zero exit status 1.\n",
      "All packages are verified.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../scripts'))\n",
    "from myFunctions import install_packages\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guitz\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### importing manipulation packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### importing torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "### importing optuna packages\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "\n",
    "### importing metrics packages\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL CLASSES DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based model for time-series tasks, supporting regression and classification.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features per time step.\n",
    "        hidden_size (int): Number of hidden units in each LSTM layer.\n",
    "        num_layers (int): Number of LSTM layers.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, target):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size // 2, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-based model for time-series tasks, supporting regression and classification.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features per time step.\n",
    "        hidden_size (int): Number of hidden units in each GRU layer.\n",
    "        num_layers (int): Number of GRU layers.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, target):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size // 2, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hidden = self.gru(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid CNN-LSTM model for time-series tasks, supporting regression and classification.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features per time step.\n",
    "        hidden_size (int): Number of hidden units in each LSTM layer.\n",
    "        num_layers (int): Number of LSTM layers.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "        conv_filters (int): Number of filters in the 1D convolutional layer.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, conv_filters, target):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_size, out_channels=conv_filters, kernel_size=3, stride=1, padding=1)\n",
    "        self.lstm = nn.LSTM(conv_filters, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size // 2, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output\n",
    "\n",
    "class CNNGRUModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid CNN-GRU model for time-series tasks, supporting regression and classification.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features per time step.\n",
    "        hidden_size (int): Number of hidden units in each GRU layer.\n",
    "        num_layers (int): Number of GRU layers.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "        conv_filters (int): Number of filters in the 1D convolutional layer.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, conv_filters, target):\n",
    "        super(CNNGRUModel, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_size, out_channels=conv_filters, kernel_size=3, stride=1, padding=1)\n",
    "        self.gru = nn.GRU(conv_filters, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size // 2, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, hidden = self.gru(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESSING DATA TO ARRAYS FOR WINDOWS SIZE AND LOOK FORWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df, exclude_columns=['date', 'day']):\n",
    "    \"\"\"\n",
    "    Escalona todas as colunas do DataFrame, exceto as especificadas em exclude_columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): O DataFrame contendo os dados.\n",
    "        exclude_columns (list): Lista de colunas que não serão escalonadas.\n",
    "\n",
    "    Returns:\n",
    "        df_scaled (pd.DataFrame): DataFrame com as colunas escalonadas.\n",
    "        scalers (dict): Dicionário contendo os escaladores para cada coluna escalonada.\n",
    "    \"\"\"\n",
    "    scalers = {}\n",
    "    columns_to_scale = [col for col in df.columns if col not in exclude_columns]\n",
    "\n",
    "    df_scaled = df.copy()\n",
    "    for col in columns_to_scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_scaled[col] = scaler.fit_transform(df[[col]])\n",
    "        scalers[col] = scaler\n",
    "\n",
    "    return df_scaled, scalers\n",
    "\n",
    "\n",
    "def data_to_array(df, window_size, target, features):\n",
    "    \"\"\"\n",
    "    Prepares X and y with targets shifted for the next day after the window.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe containing the data.\n",
    "        window_size (int): The window size (e.g., 7 days).\n",
    "        target (str): The target column name (e.g., 'close_price_target').\n",
    "        features (list): List of feature column names (e.g., ['open', 'high', 'low', 'close']).\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Input features.\n",
    "        y (np.ndarray): Target values.\n",
    "        y_dates (np.ndarray): Dates associated with the targets.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    y_dates = []\n",
    "\n",
    "    for i in range(len(df) - window_size):\n",
    "        # Access the target column directly by its name\n",
    "        target_value = df.iloc[i + window_size][target]\n",
    "        y.append(target_value)\n",
    "        y_dates.append(df.iloc[i + window_size]['date'])\n",
    "\n",
    "        # Prepare the features using the provided column names\n",
    "        X.append(df.iloc[i:i + window_size][features].values)\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(y_dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLITING DATA TO TRAIN AND TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def segment_data(df, test_size=0.15):\n",
    "    \"\"\"\n",
    "    Segments the data into training and test sets based on the test size percentage.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be segmented.\n",
    "        test_size (float): The percentage of data to be used for testing.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Training data.\n",
    "        pd.DataFrame: Testing data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculates the size of the test set\n",
    "    test_len = int(len(df) * test_size)\n",
    "\n",
    "    # Segments the data\n",
    "    train_data = df[:-test_len]  # 85% for training\n",
    "    test_data = df[-test_len:]   # 15% for testing\n",
    "\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(trial, X_train, y_train, X_test, y_test, target, window_size, look_forward, model_type, study_name, model_dir):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model using the given parameters and data.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): The current optuna trial to optimize hyperparameters.\n",
    "        X_train (ndarray): The training input data (features).\n",
    "        y_train (ndarray): The target labels for the training data.\n",
    "        X_test (ndarray): The test input data (features).\n",
    "        y_test (ndarray): The target labels for the test data.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "        window_size (int): The size of the sliding window for time-series data.\n",
    "        look_forward (int): The number of steps ahead to predict.\n",
    "        model_type (str): The type of model ('LSTM', 'GRU', 'CNN-LSTM', 'CNN-GRU').\n",
    "        study_name (str): Name of the optuna study for logging purposes.\n",
    "        model_dir (str): Directory to save the model and trial results.\n",
    "\n",
    "    Returns:\n",
    "        float: The evaluation error (either MSE or AUC depending on the task).\n",
    "    \"\"\"\n",
    "    # Set up the device for computation (GPU if available, otherwise CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Convert input data to PyTorch tensors and move them to the appropriate device\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long if 'behavior' in target else torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long if 'behavior' in target else torch.float32).to(device)\n",
    "\n",
    "    # Suggest hyperparameters using Optuna\n",
    "    epochs = trial.suggest_int('epochs',350, 1000)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 3)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 128)\n",
    "\n",
    "    if model_type in ['CNN-LSTM', 'CNN-GRU']:\n",
    "        conv_filters = trial.suggest_categorical('conv_filters', [32, 64, 128])\n",
    "\n",
    "    # Initialize the model based on the selected model type\n",
    "    if model_type == 'LSTM':\n",
    "        model = LSTMModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, target=target).to(device)\n",
    "    elif model_type == 'GRU':\n",
    "        model = GRUModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, target=target).to(device)\n",
    "    elif model_type == 'CNN-LSTM':\n",
    "        model = CNNLSTMModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, conv_filters=conv_filters, target=target).to(device)\n",
    "    elif model_type == 'CNN-GRU':\n",
    "        model = CNNGRUModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, conv_filters=conv_filters, target=target).to(device)\n",
    "\n",
    "    # Define the optimizer and the loss function (CrossEntropyLoss for classification, MSELoss for regression)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss() if 'behavior' in target else nn.MSELoss()\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize a dictionary to track loss at each epoch\n",
    "    average_losses = {epoch: [] for epoch in range(epochs)}\n",
    "\n",
    "    # Training loop with dimension adjustments\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "\n",
    "            if 'behavior' in target:\n",
    "               output = output.view(-1, 2)\n",
    "            else:\n",
    "               output = output.view(-1)\n",
    "\n",
    "            y_batch = y_batch.view(-1)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        average_losses[epoch].append(avg_loss)\n",
    "\n",
    "        # Report the loss to Optuna for pruning and optimization\n",
    "        trial.report(avg_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Print the loss every 50 epochs\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'Epoch {epoch} loss: {avg_loss}')\n",
    "\n",
    "    # Create a directory to save trial results\n",
    "    trials_dir = os.path.join(model_dir, 'trials')\n",
    "    if not os.path.exists(trials_dir):\n",
    "        os.makedirs(trials_dir)\n",
    "\n",
    "    # Optionally, plot and save the loss decay curve (this part is commented out for now)\n",
    "    # plot_loss_decay(average_losses, epochs, study_name, trials_dir, trial.number)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for X_batch, y_batch in test_loader:\n",
    "          output = model(X_batch)\n",
    "\n",
    "          if 'behavior' in target:\n",
    "              _, predicted = torch.max(output, 1)\n",
    "\n",
    "          else:\n",
    "              predicted = output.squeeze()\n",
    "              predicted = predicted.reshape(-1)\n",
    "              print(len(predicted))\n",
    "\n",
    "          predictions.append(predicted.cpu().numpy())\n",
    "          true_values.append(y_batch.cpu().numpy())\n",
    "\n",
    "    if any(p.size == 0 for p in predictions) or any(t.size == 0 for t in true_values):\n",
    "      print(f\"Error: One or more arrays in 'predictions' or 'true_values' are empty. Pruning trial {trial.number}.\")\n",
    "      raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_values = np.concatenate(true_values, axis=0)\n",
    "\n",
    "    # Calculate error (Mean Squared Error for regression or AUC for classification)\n",
    "    if 'price' in target:\n",
    "        error = mean_squared_error(true_values, predictions)\n",
    "    else:\n",
    "        error = roc_auc_score(y_test.cpu().numpy(), predictions)\n",
    "\n",
    "    # Save loss decay data to a file\n",
    "    loss_decay_file = os.path.join(trials_dir, f\"{study_name}_trial_{trial.number}_loss_decay.pkl\")\n",
    "    with open(loss_decay_file, 'wb') as f:\n",
    "        joblib.dump(average_losses, f)\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_models(df: pd.DataFrame,\n",
    "                    targets: list,\n",
    "                    featurecs: list,\n",
    "                    look_baks: list,\n",
    "                    look_forwards: list,\n",
    "                    target_dir: str = None,\n",
    "                    models: list = None,\n",
    "                    max_samples: int = 100):\n",
    "    study_results = {}\n",
    "\n",
    "    if target_dir is not None:\n",
    "        input_dir = os.path.join('..', 'data', 'hyperparameters', target_dir)\n",
    "    else:\n",
    "        input_dir = os.path.join('..', 'data', 'hyperparameters')\n",
    "\n",
    "    if not os.path.exists(input_dir):\n",
    "        os.makedirs(input_dir)\n",
    "\n",
    "    exclude_columns = ['date', 'day']\n",
    "    df_scaled, _ = scale_features(df, exclude_columns)\n",
    "    train_data, test_data = segment_data(df_scaled, test_size=0.15)\n",
    "\n",
    "    for window_size in look_backs:\n",
    "        for look_forward in look_forwards:\n",
    "            for target in targets:\n",
    "                X_train, y_train, y_dates = data_to_array(train_data.copy(),\n",
    "                                                          window_size,\n",
    "                                                          target,\n",
    "                                                          features)\n",
    "\n",
    "                X_test, y_test, _ = data_to_array(test_data.copy(),\n",
    "                                                  window_size,\n",
    "                                                  target,\n",
    "                                                  features)\n",
    "\n",
    "                if models is None:\n",
    "                    models = ['LSTM', 'GRU', 'CNN-LSTM', 'CNN-GRU']\n",
    "\n",
    "                for model_type in tqdm(models):\n",
    "                    study_name = f\"{model_type}_look_back_{window_size}_look_forward_{look_forward}_{target}\"\n",
    "                    model_dir = os.path.join(input_dir, study_name)\n",
    "\n",
    "                    if not os.path.exists(model_dir):\n",
    "                        os.makedirs(model_dir)\n",
    "\n",
    "                    # Define optimization direction\n",
    "                    direction = 'minimize' if 'price' in target else 'maximize'\n",
    "\n",
    "                    # Creating the Optuna study with a pruner\n",
    "\n",
    "                    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=350, interval_steps=1)\n",
    "                    study = optuna.create_study(direction=direction, study_name=study_name, pruner=pruner)\n",
    "                    study.optimize(\n",
    "                        lambda trial: train_evaluate_model(\n",
    "                            trial, X_train, y_train, X_test, y_test, target, window_size, look_forward, model_type, study_name, model_dir\n",
    "                        ),\n",
    "                        n_trials=max_samples\n",
    "                    )\n",
    "\n",
    "                    study_file = os.path.join(model_dir, f\"{study_name}_study.pkl\")\n",
    "                    joblib.dump(study, study_file)\n",
    "                    print(f\"{study_name} saved to {study_file}\")\n",
    "\n",
    "                    best_params = study.best_params\n",
    "                    best_trial_index = study.best_trial.number\n",
    "                    best_trial_value = study.best_value\n",
    "\n",
    "                    best_params_dict = {\n",
    "                        'best_params': best_params,\n",
    "                        'best_trial_index': best_trial_index,\n",
    "                        'best_trial_value': best_trial_value\n",
    "                    }\n",
    "                    params_file = os.path.join(model_dir, f\"{study_name}_best_params.pkl\")\n",
    "                    with open(params_file, \"wb\") as f:\n",
    "                        joblib.dump(best_params_dict, f)\n",
    "\n",
    "                    try:\n",
    "                        fig_optimization = vis.plot_optimization_history(study)\n",
    "                        fig_optimization.write_image(os.path.join(model_dir, f\"{study_name}_optimization_history.png\"))\n",
    "\n",
    "                        fig_importances = vis.plot_param_importances(study)\n",
    "                        fig_importances.write_image(os.path.join(model_dir, f\"{study_name}_param_importances.png\"))\n",
    "\n",
    "                        fig_slice = vis.plot_slice(study)\n",
    "                        fig_slice.write_image(os.path.join(model_dir, f\"{study_name}_slice_plot.png\"))\n",
    "                        print(f\"Images saved to {model_dir}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in plotting images for {study_name}: {e}\")\n",
    "\n",
    "                    print(f\"Saved to {model_dir}:\")\n",
    "                    print(f\"Best hyperparameters for {model_type}, lookback: {window_size}, look forward: {look_forward}, target {target}: {best_params}\")\n",
    "                    print(f\"Best trial index: {best_trial_index}, Best trial value: {best_trial_value}\")\n",
    "\n",
    "                    study_results[study_name] = {\n",
    "                        \"study\": study,\n",
    "                        \"best_params\": best_params,\n",
    "                        \"best_trial_index\": best_trial_index,\n",
    "                        \"best_trial_value\": best_trial_value,\n",
    "                        \"directory\": model_dir\n",
    "                    }\n",
    "\n",
    "    return study_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading data to optuna\n",
    "input_dir = os.path.join('..', 'data', 'processed')\n",
    "features_name = 'D1_features.pkl'\n",
    "input_dir_features = os.path.join('..', 'data', 'features')\n",
    "\n",
    "D1_df = pd.read_parquet(os.path.join(input_dir, 'D1_df.parquet'))\n",
    "M15_df = pd.read_parquet(os.path.join(input_dir, 'M15_df.parquet'))\n",
    "D1_features = joblib.load(os.path.join(input_dir_features, 'D1_features.pkl'))\n",
    "M15_features = joblib.load(os.path.join(input_dir_features, 'M15_features.pkl'))\n",
    "\n",
    "models = ['LSTM', 'GRU', 'CNN-LSTM', 'CNN-GRU'] # define one or more models\n",
    "targets = ['close_price_target', 'open_price_target', 'behavior_target']\n",
    "windows = [7, 15, 30, 45, 60] \n",
    "look_forwards = [1]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open_AGFS</th>\n",
       "      <th>high_AGFS</th>\n",
       "      <th>low_AGFS</th>\n",
       "      <th>close_AGFS</th>\n",
       "      <th>tick_volume_AGFS</th>\n",
       "      <th>spread_AGFS</th>\n",
       "      <th>real_volume_AGFS</th>\n",
       "      <th>open_BGI$</th>\n",
       "      <th>high_BGI$</th>\n",
       "      <th>...</th>\n",
       "      <th>EMA55_GOLD11</th>\n",
       "      <th>EMA9_IBOV</th>\n",
       "      <th>EMA21_IBOV</th>\n",
       "      <th>EMA55_IBOV</th>\n",
       "      <th>EMA9_DOL$</th>\n",
       "      <th>EMA21_DOL$</th>\n",
       "      <th>EMA55_DOL$</th>\n",
       "      <th>close_price_target</th>\n",
       "      <th>open_price_target</th>\n",
       "      <th>behavior_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-06-02 09:00:00</td>\n",
       "      <td>1767.0</td>\n",
       "      <td>1767.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>320.52</td>\n",
       "      <td>321.21</td>\n",
       "      <td>...</td>\n",
       "      <td>9.380451</td>\n",
       "      <td>111438.765770</td>\n",
       "      <td>111480.083554</td>\n",
       "      <td>111455.854537</td>\n",
       "      <td>5597.257017</td>\n",
       "      <td>5589.642700</td>\n",
       "      <td>5564.905769</td>\n",
       "      <td>319.29</td>\n",
       "      <td>319.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-06-02 09:15:00</td>\n",
       "      <td>1767.0</td>\n",
       "      <td>1767.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.73</td>\n",
       "      <td>319.73</td>\n",
       "      <td>...</td>\n",
       "      <td>9.382935</td>\n",
       "      <td>111423.012616</td>\n",
       "      <td>111469.166867</td>\n",
       "      <td>111452.431161</td>\n",
       "      <td>5593.956413</td>\n",
       "      <td>5588.834637</td>\n",
       "      <td>5565.471777</td>\n",
       "      <td>319.29</td>\n",
       "      <td>319.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-06-02 09:30:00</td>\n",
       "      <td>1767.0</td>\n",
       "      <td>1767.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.73</td>\n",
       "      <td>319.73</td>\n",
       "      <td>...</td>\n",
       "      <td>9.385330</td>\n",
       "      <td>111410.410093</td>\n",
       "      <td>111459.242606</td>\n",
       "      <td>111449.130048</td>\n",
       "      <td>5588.428931</td>\n",
       "      <td>5586.787761</td>\n",
       "      <td>5565.502035</td>\n",
       "      <td>319.34</td>\n",
       "      <td>319.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-06-02 09:45:00</td>\n",
       "      <td>1767.0</td>\n",
       "      <td>1767.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.98</td>\n",
       "      <td>319.98</td>\n",
       "      <td>...</td>\n",
       "      <td>9.387640</td>\n",
       "      <td>111400.328074</td>\n",
       "      <td>111450.220551</td>\n",
       "      <td>111445.946832</td>\n",
       "      <td>5584.584345</td>\n",
       "      <td>5585.189419</td>\n",
       "      <td>5565.634320</td>\n",
       "      <td>320.13</td>\n",
       "      <td>319.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-06-02 10:00:00</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>1778.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>9986.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6702403.0</td>\n",
       "      <td>319.39</td>\n",
       "      <td>320.57</td>\n",
       "      <td>...</td>\n",
       "      <td>9.389510</td>\n",
       "      <td>111577.062459</td>\n",
       "      <td>111526.018683</td>\n",
       "      <td>111475.877302</td>\n",
       "      <td>5583.240876</td>\n",
       "      <td>5584.523744</td>\n",
       "      <td>5566.071201</td>\n",
       "      <td>320.52</td>\n",
       "      <td>320.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time  open_AGFS  high_AGFS  low_AGFS  close_AGFS  \\\n",
       "0 2022-06-02 09:00:00     1767.0     1767.0    1763.0      1763.0   \n",
       "1 2022-06-02 09:15:00     1767.0     1767.0    1763.0      1763.0   \n",
       "2 2022-06-02 09:30:00     1767.0     1767.0    1763.0      1763.0   \n",
       "3 2022-06-02 09:45:00     1767.0     1767.0    1763.0      1763.0   \n",
       "4 2022-06-02 10:00:00     1764.0     1778.0    1763.0      1777.0   \n",
       "\n",
       "   tick_volume_AGFS  spread_AGFS  real_volume_AGFS  open_BGI$  high_BGI$  ...  \\\n",
       "0               0.0          0.0               0.0     320.52     321.21  ...   \n",
       "1               0.0          0.0               0.0     319.73     319.73  ...   \n",
       "2               0.0          0.0               0.0     319.73     319.73  ...   \n",
       "3               0.0          0.0               0.0     319.98     319.98  ...   \n",
       "4            9986.0          0.0         6702403.0     319.39     320.57  ...   \n",
       "\n",
       "   EMA55_GOLD11      EMA9_IBOV     EMA21_IBOV     EMA55_IBOV    EMA9_DOL$  \\\n",
       "0      9.380451  111438.765770  111480.083554  111455.854537  5597.257017   \n",
       "1      9.382935  111423.012616  111469.166867  111452.431161  5593.956413   \n",
       "2      9.385330  111410.410093  111459.242606  111449.130048  5588.428931   \n",
       "3      9.387640  111400.328074  111450.220551  111445.946832  5584.584345   \n",
       "4      9.389510  111577.062459  111526.018683  111475.877302  5583.240876   \n",
       "\n",
       "    EMA21_DOL$   EMA55_DOL$  close_price_target  open_price_target  \\\n",
       "0  5589.642700  5564.905769              319.29             319.73   \n",
       "1  5588.834637  5565.471777              319.29             319.73   \n",
       "2  5586.787761  5565.502035              319.34             319.98   \n",
       "3  5585.189419  5565.634320              320.13             319.39   \n",
       "4  5584.523744  5566.071201              320.52             320.47   \n",
       "\n",
       "   behavior_target  \n",
       "0                0  \n",
       "1                0  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "\n",
       "[5 rows x 172 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M15_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the optimize_models function with acess to a GPU otherwise it's definetly not going to work\n",
    "study_results = optimize_models(D1_df,\n",
    "                                targets=targets,\n",
    "                                features=D1_features, \n",
    "                                look_backs=windows, \n",
    "                                look_forwards=look_forwards,\n",
    "                                max_samples=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
