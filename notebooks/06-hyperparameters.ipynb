{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTAÇÕES NECESSÁRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages: ['numpy', 'pandas', 'scikit-learn', 'joblib', 'pyarrow', 'fastparquet', 'plotly', 'matplotlib', 'MetaTrader5', 'tabulate', 'optuna', 'torch', 'tqdm', 'shap', 'kaleido']\n",
      "numpy is already installed.\n",
      "pandas is already installed.\n",
      "scikit-learn is already installed.\n",
      "joblib is already installed.\n",
      "pyarrow is already installed.\n",
      "fastparquet is already installed.\n",
      "plotly is already installed.\n",
      "matplotlib is already installed.\n",
      "MetaTrader5 is already installed.\n",
      "tabulate is already installed.\n",
      "optuna is already installed.\n",
      "torch is already installed.\n",
      "tqdm is already installed.\n",
      "shap is already installed.\n",
      "kaleido is already installed.\n",
      "All packages are verified.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../scripts'))\n",
    "from myFunctions import install_packages\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages: ['numpy', 'pandas', 'scikit-learn', 'joblib', 'pyarrow', 'fastparquet', 'plotly', 'matplotlib', 'MetaTrader5', 'tabulate', 'optuna', 'torch', 'tqdm', 'shap', 'kaleido']\n",
      "numpy is already installed.\n",
      "pandas is already installed.\n",
      "scikit-learn is already installed.\n",
      "joblib is already installed.\n",
      "pyarrow is already installed.\n",
      "fastparquet is already installed.\n",
      "plotly is already installed.\n",
      "matplotlib is already installed.\n",
      "MetaTrader5 is already installed.\n",
      "tabulate is already installed.\n",
      "optuna is already installed.\n",
      "torch is already installed.\n",
      "tqdm is already installed.\n",
      "shap is already installed.\n",
      "kaleido is already installed.\n",
      "All packages are verified.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import os\n",
    "\n",
    "def install_packages():\n",
    "    required_packages = [\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"joblib\",\n",
    "        \"pyarrow\",\n",
    "        \"fastparquet\",\n",
    "        \"plotly\",\n",
    "        \"matplotlib\",\n",
    "        \"MetaTrader5\",\n",
    "        \"tabulate\",\n",
    "        \"optuna\",\n",
    "        \"torch\",\n",
    "        \"tqdm\",\n",
    "        \"shap\",\n",
    "        \"kaleido\"\n",
    "    ]\n",
    "    \n",
    "    print(f'Installing required packages: {required_packages}')\n",
    "    # Checking installed packages\n",
    "    installed_packages = {pkg.key for pkg in pkg_resources.working_set}\n",
    "    \n",
    "    # Install missing packages\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            if package.lower() not in installed_packages:\n",
    "                print(f\"Installing {package}...\")\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            else:\n",
    "                print(f\"{package} is already installed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error installing {package}: {e}\")\n",
    "            continue  # Continue with other packages, log the error but don't stop the process\n",
    "    \n",
    "    print(\"All packages are verified.\")\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing required packages\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import kaleido\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL CLASSES DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based model for time-series tasks, supporting regression and classification.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features per time step.\n",
    "        hidden_size (int): Number of hidden units in each LSTM layer.\n",
    "        num_layers (int): Number of LSTM layers.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, target):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size // 2, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-based model for time-series tasks, supporting regression and classification.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features per time step.\n",
    "        hidden_size (int): Number of hidden units in each GRU layer.\n",
    "        num_layers (int): Number of GRU layers.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, target):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size // 2, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hidden = self.gru(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid CNN-LSTM model for time-series tasks, supporting regression and classification.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features per time step.\n",
    "        hidden_size (int): Number of hidden units in each LSTM layer.\n",
    "        num_layers (int): Number of LSTM layers.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "        conv_filters (int): Number of filters in the 1D convolutional layer.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, conv_filters, target):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_size, out_channels=conv_filters, kernel_size=3, stride=1, padding=1)\n",
    "        self.lstm = nn.LSTM(conv_filters, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size // 2, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output\n",
    "\n",
    "class CNNGRUModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid CNN-GRU model for time-series tasks, supporting regression and classification.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features per time step.\n",
    "        hidden_size (int): Number of hidden units in each GRU layer.\n",
    "        num_layers (int): Number of GRU layers.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "        conv_filters (int): Number of filters in the 1D convolutional layer.\n",
    "        target (str): Target type ('price' for regression, 'behavior' for classification).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, conv_filters, target):\n",
    "        super(CNNGRUModel, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_size, out_channels=conv_filters, kernel_size=3, stride=1, padding=1)\n",
    "        self.gru = nn.GRU(conv_filters, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size // 2, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, hidden = self.gru(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESSING DATA TO ARRAYS FOR WINDOWS SIZE AND LOOK FORWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df, exclude_columns=['date', 'day']):\n",
    "    \"\"\"\n",
    "    Escalona todas as colunas do DataFrame, exceto as especificadas em exclude_columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): O DataFrame contendo os dados.\n",
    "        exclude_columns (list): Lista de colunas que não serão escalonadas.\n",
    "\n",
    "    Returns:\n",
    "        df_scaled (pd.DataFrame): DataFrame com as colunas escalonadas.\n",
    "        scalers (dict): Dicionário contendo os escaladores para cada coluna escalonada.\n",
    "    \"\"\"\n",
    "    scalers = {}\n",
    "    columns_to_scale = [col for col in df.columns if col not in exclude_columns]\n",
    "    \n",
    "    df_scaled = df.copy()\n",
    "    for col in columns_to_scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_scaled[col] = scaler.fit_transform(df[[col]])\n",
    "        scalers[col] = scaler\n",
    "    \n",
    "    return df_scaled, scalers\n",
    "\n",
    "\n",
    "def data_to_array(df, window_size, target, features):\n",
    "    \"\"\"\n",
    "    Prepares X and y with targets shifted for the next day after the window.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe containing the data.\n",
    "        window_size (int): The window size (e.g., 7 days).\n",
    "        target (str): The target column name (e.g., 'close_price_target').\n",
    "        features (list): List of feature column names (e.g., ['open', 'high', 'low', 'close']).\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Input features.\n",
    "        y (np.ndarray): Target values.\n",
    "        y_dates (np.ndarray): Dates associated with the targets.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    y_dates = []\n",
    "\n",
    "    for i in range(len(df) - window_size):\n",
    "        # Access the target column directly by its name\n",
    "        target_value = df.iloc[i + window_size][target]\n",
    "        y.append(target_value)\n",
    "        y_dates.append(df.iloc[i + window_size]['date'])\n",
    "        \n",
    "        # Prepare the features using the provided column names\n",
    "        X.append(df.iloc[i:i + window_size][features].values)\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(y_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLITING DATA TO TRAIN AND TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_data(df, test_size=0.15):\n",
    "    \"\"\"\n",
    "    Segments the data into training and test sets based on the test size percentage.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be segmented.\n",
    "        test_size (float): The percentage of data to be used for testing.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Training data.\n",
    "        pd.DataFrame: Testing data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculates the size of the test set\n",
    "    test_len = int(len(df) * test_size)\n",
    "    \n",
    "    # Segments the data\n",
    "    train_data = df[:-test_len]  # 85% for training\n",
    "    test_data = df[-test_len:]   # 15% for testing\n",
    "    \n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(trial, X_train, y_train, X_test, y_test, target, window_size, look_forward, model_type, study_name, model_dir):\n",
    "    \"\"\"\n",
    "    Optimizes models using Optuna for hyperparameter tuning. Optimizes based on AUC score for binary classification tasks.\n",
    "\n",
    "    Args:\n",
    "        X_train (DataFrame): The training data features.\n",
    "        y_train (DataFrame): The training data target labels.\n",
    "        X_test (DataFrame): The test data features.\n",
    "        y_test (DataFrame): The test data target labels.\n",
    "        target (str): The target variable ('price' or 'behavior').\n",
    "        window_size (int): Size of the window for input data.\n",
    "        look_forward (int): Look-ahead for prediction.\n",
    "        model_type (str): Type of the model to train (LSTM, GRU, etc.).\n",
    "        study_name (str): Name of the study for saving results.\n",
    "        model_dir (str): Directory to save model-related files.\n",
    "\n",
    "    Returns:\n",
    "        float: The evaluation error (e.g., AUC score or MSE).\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long if 'behavior' in target else torch.float32)  # Changed to long for behavior\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long if 'behavior' in target else torch.float32)  # Changed to long for behavior\n",
    "\n",
    "    epochs = trial.suggest_int('epochs', 200, 1000)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 256)\n",
    "\n",
    "    if model_type in ['CNN-LSTM', 'CNN-GRU']:\n",
    "        conv_filters = trial.suggest_categorical('conv_filters', [32, 64, 128])\n",
    "\n",
    "    # Initialize model based on type\n",
    "    if model_type == 'LSTM':\n",
    "        model = LSTMModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, target=target)\n",
    "    elif model_type == 'GRU':\n",
    "        model = GRUModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, target=target)\n",
    "    elif model_type == 'CNN-LSTM':\n",
    "        model = CNNLSTMModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, conv_filters=conv_filters, target=target)\n",
    "    elif model_type == 'CNN-GRU':\n",
    "        model = CNNGRUModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, conv_filters=conv_filters, target=target)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Select criterion based on target\n",
    "    criterion = nn.CrossEntropyLoss() if 'behavior' in target else nn.MSELoss()\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "    average_losses = {epoch: [] for epoch in range(epochs)}\n",
    "\n",
    "    # Loop de treinamento\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            \n",
    "            if target == 'behavior_target':  # Flatten para CrossEntropyLoss\n",
    "                output = output.view(-1, 2)  # Garantindo que a saída tem 2 classes para cada batch\n",
    "                y_batch = y_batch.view(-1)  # Flatten y_batch para ser compatível com CrossEntropy\n",
    "\n",
    "            loss = criterion(output, y_batch)\n",
    "           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        average_losses[epoch].append(avg_loss)\n",
    "        if epoch  %  50  == 0: \n",
    "            print(f'{epoch} loss: {avg_loss}')\n",
    "\n",
    "    # Evaluation on test data\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            output = model(X_batch)\n",
    "            if 'behavior' in target:\n",
    "                _, predicted = torch.max(output, 1)  # Get the class with highest probability\n",
    "            else:\n",
    "                predicted = output\n",
    "                \n",
    "            predictions.append(predicted.numpy())\n",
    "            true_values.append(y_batch.numpy())\n",
    "                        \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_values = np.concatenate(true_values, axis=0)\n",
    "\n",
    "    # Calculate error based on target\n",
    "    if 'price' in target:\n",
    "        error = mean_squared_error(true_values, predictions)\n",
    "    else:\n",
    "        error = roc_auc_score(y_test.numpy(), predictions)\n",
    "\n",
    "    # Save loss decay file\n",
    "    loss_decay_file = os.path.join(model_dir, f\"{study_name}_loss_decay.pkl\")\n",
    "    with open(loss_decay_file, 'wb') as f:\n",
    "        joblib.dump(average_losses, f)\n",
    "\n",
    "    # Plot and save loss decay graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(epochs), [np.mean(loss) for loss in average_losses.values()], label='Average Training Loss')\n",
    "    plt.title(f'Loss Decay Over Epochs for {study_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot as an image\n",
    "    loss_plot_file = os.path.join(model_dir, f\"{study_name}_loss_decay_plot.png\")\n",
    "    plt.savefig(loss_plot_file)\n",
    "    plt.close()\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_models(df: pd.DataFrame, \n",
    "                    targets: list, \n",
    "                    features: list, \n",
    "                    look_backs: list, \n",
    "                    look_forwards: list, \n",
    "                    target_dir: str = None, \n",
    "                    models: list = None, \n",
    "                    max_samples: int= 100):\n",
    "    \"\"\"\n",
    "    Optimizes models using Optuna for hyperparameter tuning. Optimizes based on AUC or RMSE depending on the target type.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The data to be used for training.\n",
    "        targets (list): List of target columns to be used in training.\n",
    "        target_dir (str): The name of the directory to save optuna studies \n",
    "        features (list): List of features to be used for training.\n",
    "        look_backs (list): List of window sizes for the sliding window approach.\n",
    "        look_forwards (list): List of look-forward values for forecasting.\n",
    "        max_samples (int): The maximum number of Optuna trials to perform.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing study results, including best parameters and model directories.\n",
    "    \"\"\"\n",
    "    study_results = {}\n",
    "    \n",
    "    if target_dir is not None:\n",
    "        input_dir = os.path.join('..', 'data', 'models', target_dir).replace(\"/\", \"\\\\\")\n",
    "    else:\n",
    "        input_dir = os.path.join('..', 'data', 'models').replace(\"/\", \"\\\\\")\n",
    "    \n",
    "    if not os.path.exists(input_dir):\n",
    "        os.makedirs(input_dir)\n",
    "    \n",
    "    exclude_columns = ['date', 'day']\n",
    "    df_scaled, _ = scale_features(df, exclude_columns)\n",
    "    train_data, test_data = segment_data(df_scaled, test_size=0.15)\n",
    "    \n",
    "    for window_size in look_backs:\n",
    "        for look_forward in look_forwards:\n",
    "            for target in targets:\n",
    "                X_train, y_train, y_dates = data_to_array(train_data.copy(), \n",
    "                                                          window_size,\n",
    "                                                          target, \n",
    "                                                          features)\n",
    "\n",
    "                X_test, y_test, _ = data_to_array(test_data.copy(), \n",
    "                                                  window_size, \n",
    "                                                  target, \n",
    "                                                  features)\n",
    "\n",
    "                if models == None:\n",
    "                    models = models = ['LSTM', 'GRU', 'CNN-LSTM', 'CNN-GRU']\n",
    "                \n",
    "                for model_type in tqdm(models):\n",
    "                    study_name = f\"{model_type}_look_back_{window_size}_look_forward_{look_forward}_{target}\"\n",
    "                    model_dir = os.path.join(input_dir, study_name).replace(\"/\", \"\\\\\")\n",
    "                    \n",
    "                    if not os.path.exists(model_dir):\n",
    "                        os.makedirs(model_dir)\n",
    "                    \n",
    "                    # Define optimization direction\n",
    "                    direction = 'minimize' if 'price' in target else 'maximize'\n",
    "                        \n",
    "                    # Creating the Optuna study\n",
    "                    study = optuna.create_study(direction=direction, study_name=study_name)\n",
    "                    study.optimize(\n",
    "                        lambda trial: train_evaluate_model(\n",
    "                            trial, X_train, y_train, X_test, y_test, target, window_size, look_forward, model_type, study_name, model_dir\n",
    "                        ),\n",
    "                        n_trials=max_samples\n",
    "                    )\n",
    "\n",
    "                    study_file = os.path.join(model_dir, f\"{study_name}_study.pkl\").replace(\"/\", \"\\\\\")\n",
    "                    joblib.dump(study, study_file)\n",
    "                    print(f'{study_name} saved to {study_file}')\n",
    "\n",
    "                    best_params = study.best_params\n",
    "                    best_trial_index = study.best_trial.number \n",
    "                    best_trial_value = study.best_value  \n",
    "\n",
    "                    best_params_dict = {\n",
    "                        'best_params': best_params,\n",
    "                        'best_trial_index': best_trial_index,\n",
    "                        'best_trial_value': best_trial_value\n",
    "                    }\n",
    "                    params_file = os.path.join(model_dir, f\"{study_name}_best_params.pkl\").replace(\"/\", \"\\\\\")\n",
    "                    with open(params_file, \"wb\") as f:\n",
    "                        joblib.dump(best_params_dict, f)\n",
    "\n",
    "                    try:\n",
    "                        fig_optimization = vis.plot_optimization_history(study)\n",
    "                        fig_optimization.write_image(os.path.join(model_dir, f\"{study_name}_optimization_history.png\"))\n",
    "\n",
    "                        fig_importances = vis.plot_param_importances(study)\n",
    "                        fig_importances.write_image(os.path.join(model_dir, f\"{study_name}_param_importances.png\"))\n",
    "\n",
    "                        fig_slice = vis.plot_slice(study)\n",
    "                        fig_slice.write_image(os.path.join(model_dir, f\"{study_name}_slice_plot.png\"))\n",
    "                        print(f\"Images saved to {model_dir}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in plotting images for {study_name}: {e}\")\n",
    "\n",
    "\n",
    "                    print(f\"Saved to {model_dir}:\")\n",
    "                    print(f\"Best hyperparameters for {model_type}, lookback: {window_size}, look forward: {look_forward}, target {target}: {best_params}\")\n",
    "                    print(f\"Best trial index: {best_trial_index}, Best trial value: {best_trial_value}\")\n",
    "\n",
    "                    study_results[study_name] = {\n",
    "                        \"study\": study,\n",
    "                        \"best_params\": best_params,\n",
    "                        \"best_trial_index\": best_trial_index,\n",
    "                        \"best_trial_value\": best_trial_value,\n",
    "                        \"directory\": model_dir\n",
    "                    }\n",
    "\n",
    "    return study_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s][I 2024-12-24 16:22:57,942] A new study created in memory with name: LSTM_look_back_7_look_forward_1_behavior_target\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.201784943698827 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.7013999462127686\n",
      "50 loss: 0.5425910472869873\n",
      "100 loss: 0.3855906628072262\n",
      "150 loss: 0.23524606488645078\n",
      "200 loss: 0.09301215894520283\n",
      "250 loss: 0.02848733770661056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 16:25:06,795] Trial 0 finished with value: 0.5456745311554749 and parameters: {'epochs': 281, 'batch_size': 128, 'learning_rate': 0.00015664709706185215, 'dropout': 0.201784943698827, 'num_layers': 1, 'hidden_size': 68}. Best is trial 0 with value: 0.5456745311554749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6871935655088985\n",
      "50 loss: 0.6729646745850059\n",
      "100 loss: 0.6807663756258348\n",
      "150 loss: 0.6642580312841079\n",
      "200 loss: 0.6394934478928062\n",
      "250 loss: 0.6213329329210169\n",
      "300 loss: 0.5858830441446865\n",
      "350 loss: 0.5781002342700958\n",
      "400 loss: 0.5571898888139164\n",
      "450 loss: 0.5246055774828967\n",
      "500 loss: 0.4849419216899311\n",
      "550 loss: 0.4695916202138452\n",
      "600 loss: 0.6062657727914698\n",
      "650 loss: 0.6133950931184432\n",
      "700 loss: 0.48248228781363545\n",
      "750 loss: 0.45475513970150666\n",
      "800 loss: 0.52170270856689\n",
      "850 loss: 0.4265202844844145\n",
      "900 loss: 0.42879685496582703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 16:56:40,690] Trial 1 finished with value: 0.5241984271022383 and parameters: {'epochs': 947, 'batch_size': 32, 'learning_rate': 0.00021851114052870164, 'dropout': 0.3103234167300018, 'num_layers': 3, 'hidden_size': 146}. Best is trial 0 with value: 0.5456745311554749.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3150173939721488 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.689348664548662\n",
      "50 loss: 0.6544231308831109\n",
      "100 loss: 0.6784626377953423\n",
      "150 loss: 0.6734987431102328\n",
      "200 loss: 0.6581983963648478\n",
      "250 loss: 0.5955882800949944\n",
      "300 loss: 0.5048624376455942\n",
      "350 loss: 0.5777310050196118\n",
      "400 loss: 0.480887265668975\n",
      "450 loss: 0.4288746962944667\n",
      "500 loss: 0.404003332886431\n",
      "550 loss: 0.27963235560390687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 17:35:02,027] Trial 2 finished with value: 0.5093768905021173 and parameters: {'epochs': 585, 'batch_size': 64, 'learning_rate': 0.0007544725012968957, 'dropout': 0.3150173939721488, 'num_layers': 1, 'hidden_size': 149}. Best is trial 0 with value: 0.5456745311554749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6876695487234328\n",
      "50 loss: 0.6791253354814317\n",
      "100 loss: 0.6859137879477607\n",
      "150 loss: 0.6793382631407844\n",
      "200 loss: 0.685829136106703\n",
      "250 loss: 0.676429397530026\n",
      "300 loss: 0.6858173741234673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 17:58:28,671] Trial 3 finished with value: 0.5 and parameters: {'epochs': 302, 'batch_size': 64, 'learning_rate': 0.0012650154833088736, 'dropout': 0.22492228722201085, 'num_layers': 3, 'hidden_size': 182}. Best is trial 0 with value: 0.5456745311554749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6850691795349121\n",
      "50 loss: 0.6830156564712524\n",
      "100 loss: 0.684350049495697\n",
      "150 loss: 0.6856898188591003\n",
      "200 loss: 0.6840284824371338\n",
      "250 loss: 0.6802048802375793\n",
      "300 loss: 0.6855558156967163\n",
      "350 loss: 0.6841801643371582\n",
      "400 loss: 0.6837492465972901\n",
      "450 loss: 0.6840447425842285\n",
      "500 loss: 0.6841183304786682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 18:39:53,926] Trial 4 finished with value: 0.5 and parameters: {'epochs': 539, 'batch_size': 128, 'learning_rate': 0.0040768774124428, 'dropout': 0.4194506807097166, 'num_layers': 3, 'hidden_size': 53}. Best is trial 0 with value: 0.5456745311554749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6864112734794616\n",
      "50 loss: 0.6748087882995606\n",
      "100 loss: 0.6756950497627259\n",
      "150 loss: 0.673706591129303\n",
      "200 loss: 0.6755930542945862\n",
      "250 loss: 0.6644454598426819\n",
      "300 loss: 0.6842204332351685\n",
      "350 loss: 0.6822952151298523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 19:11:51,196] Trial 5 finished with value: 0.5 and parameters: {'epochs': 398, 'batch_size': 128, 'learning_rate': 0.0011249085078378048, 'dropout': 0.23965903789118323, 'num_layers': 3, 'hidden_size': 199}. Best is trial 0 with value: 0.5456745311554749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6872654942905202\n",
      "50 loss: 0.6715211903347689\n",
      "100 loss: 0.6599549931638381\n",
      "150 loss: 0.6655796275419348\n",
      "200 loss: 0.6407742675612954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 19:14:35,793] Trial 6 finished with value: 0.5390199637023593 and parameters: {'epochs': 247, 'batch_size': 32, 'learning_rate': 0.0002096660312765543, 'dropout': 0.31809683694404906, 'num_layers': 3, 'hidden_size': 52}. Best is trial 0 with value: 0.5456745311554749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6968512415885926\n",
      "50 loss: 0.6856310606002808\n",
      "100 loss: 0.6834131240844726\n",
      "150 loss: 0.6841611623764038\n",
      "200 loss: 0.6843810558319092\n",
      "250 loss: 0.6840636134147644\n",
      "300 loss: 0.6840984106063843\n",
      "350 loss: 0.6841281890869141\n",
      "400 loss: 0.6841092228889465\n",
      "450 loss: 0.6841092944145203\n",
      "500 loss: 0.6841092944145203\n",
      "550 loss: 0.6841093182563782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 20:11:10,693] Trial 7 finished with value: 0.5 and parameters: {'epochs': 584, 'batch_size': 128, 'learning_rate': 0.004327501557258895, 'dropout': 0.47302116974270975, 'num_layers': 3, 'hidden_size': 148}. Best is trial 0 with value: 0.5456745311554749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6888724214890424\n",
      "50 loss: 0.6868319160798017\n",
      "100 loss: 0.6868581841973698\n",
      "150 loss: 0.6868549971019521\n",
      "200 loss: 0.6868551759158864\n",
      "250 loss: 0.686855270582087\n",
      "300 loss: 0.6868552740882424\n",
      "350 loss: 0.6868552916190204\n",
      "400 loss: 0.6868552916190204\n",
      "450 loss: 0.6868552881128648\n",
      "500 loss: 0.6868553056436426\n",
      "550 loss: 0.6868553091497982\n",
      "600 loss: 0.6868553091497982\n",
      "650 loss: 0.6868553231744206\n",
      "700 loss: 0.6868553266805761\n",
      "750 loss: 0.6868553231744206\n",
      "800 loss: 0.6868553231744206\n",
      "850 loss: 0.6868553231744206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 20:18:47,297] Trial 8 finished with value: 0.5 and parameters: {'epochs': 856, 'batch_size': 32, 'learning_rate': 0.009035920888542305, 'dropout': 0.3564068490287404, 'num_layers': 3, 'hidden_size': 30}. Best is trial 0 with value: 0.5456745311554749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6896079659461976\n",
      "50 loss: 0.6560905337333679\n",
      "100 loss: 0.622877436876297\n",
      "150 loss: 0.5347671270370483\n",
      "200 loss: 0.633338475227356\n",
      "250 loss: 0.4402512773871422\n",
      "300 loss: 0.4784726083278656\n",
      "350 loss: 0.41629285663366317\n",
      "400 loss: 0.44337519705295564\n",
      "450 loss: 0.41137698143720625\n",
      "500 loss: 0.43752133250236513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 20:36:31,672] Trial 9 finished with value: 0.49213551119177257 and parameters: {'epochs': 502, 'batch_size': 128, 'learning_rate': 0.0006509696956745281, 'dropout': 0.3126170456017328, 'num_layers': 2, 'hidden_size': 125}. Best is trial 0 with value: 0.5456745311554749.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2086195164250025 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6833595633506775\n",
      "50 loss: 0.5313157737255096\n",
      "100 loss: 0.3814586475491524\n",
      "150 loss: 0.2129868444055319\n",
      "200 loss: 0.10047949282452465\n",
      "250 loss: 0.03936850321479142\n",
      "300 loss: 0.017461604601703585\n",
      "350 loss: 0.00878512414637953\n",
      "400 loss: 0.005111124052200466\n",
      "450 loss: 0.0032236087514320388\n",
      "500 loss: 0.002141258184565231\n",
      "550 loss: 0.0014821512020716909\n",
      "600 loss: 0.0010557131958194078\n",
      "650 loss: 0.0007677822628465947\n",
      "700 loss: 0.0005661830662575084\n",
      "750 loss: 0.0004329407965997234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 20:44:11,429] Trial 10 finished with value: 0.5120992135511192 and parameters: {'epochs': 765, 'batch_size': 128, 'learning_rate': 0.00010636723493748743, 'dropout': 0.2086195164250025, 'num_layers': 1, 'hidden_size': 96}. Best is trial 0 with value: 0.5456745311554749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6865486152031842\n",
      "50 loss: 0.6689434472252341\n",
      "100 loss: 0.6656815339537228\n",
      "150 loss: 0.6538089303409352\n",
      "200 loss: 0.6103128089624292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 20:47:02,238] Trial 11 finished with value: 0.5632183908045977 and parameters: {'epochs': 207, 'batch_size': 32, 'learning_rate': 0.00024165540218086285, 'dropout': 0.38409330465709024, 'num_layers': 2, 'hidden_size': 70}. Best is trial 11 with value: 0.5632183908045977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.689954070483937\n",
      "50 loss: 0.6778890595716589\n",
      "100 loss: 0.6729509304551518\n",
      "150 loss: 0.6131114959716797\n",
      "200 loss: 0.5966682784697589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 20:56:33,508] Trial 12 finished with value: 0.5012099213551119 and parameters: {'epochs': 207, 'batch_size': 32, 'learning_rate': 0.0002967683406391902, 'dropout': 0.3913247924133384, 'num_layers': 2, 'hidden_size': 246}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.45714153393949586 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6914013974806842\n",
      "50 loss: 0.6433698219411513\n",
      "100 loss: 0.6681642777779523\n",
      "150 loss: 0.5020916742437026\n",
      "200 loss: 0.4360026454224306\n",
      "250 loss: 0.26485676274580117\n",
      "300 loss: 0.26313937543069615\n",
      "350 loss: 0.09628543044056963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 21:02:36,162] Trial 13 finished with value: 0.5620084694494857 and parameters: {'epochs': 392, 'batch_size': 32, 'learning_rate': 0.00011144528643721851, 'dropout': 0.45714153393949586, 'num_layers': 1, 'hidden_size': 88}. Best is trial 11 with value: 0.5632183908045977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6869013274417204\n",
      "50 loss: 0.6781459100106183\n",
      "100 loss: 0.68189735622967\n",
      "150 loss: 0.6674691368551815\n",
      "200 loss: 0.6549122964634615\n",
      "250 loss: 0.6201791412690106\n",
      "300 loss: 0.6495984687524683\n",
      "350 loss: 0.6366474383017596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 21:11:27,736] Trial 14 finished with value: 0.5384150030248034 and parameters: {'epochs': 397, 'batch_size': 32, 'learning_rate': 0.0004177820481970677, 'dropout': 0.49446002674348255, 'num_layers': 2, 'hidden_size': 102}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.44576895129488053 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6918494175462162\n",
      "50 loss: 0.6260647563373342\n",
      "100 loss: 0.5741557128289166\n",
      "150 loss: 0.49944524204029755\n",
      "200 loss: 0.4082460552453995\n",
      "250 loss: 0.37769245400148277\n",
      "300 loss: 0.2285959194688236\n",
      "350 loss: 0.17261778256472418\n",
      "400 loss: 0.12824966736576138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 21:13:32,178] Trial 15 finished with value: 0.5629159104658198 and parameters: {'epochs': 409, 'batch_size': 32, 'learning_rate': 0.00010191941564191392, 'dropout': 0.44576895129488053, 'num_layers': 1, 'hidden_size': 19}. Best is trial 11 with value: 0.5632183908045977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6871614911976982\n",
      "50 loss: 0.6750124096870422\n",
      "100 loss: 0.6746636278489057\n",
      "150 loss: 0.6658101712956148\n",
      "200 loss: 0.6577737296328825\n",
      "250 loss: 0.6463834923856399\n",
      "300 loss: 0.6291487041641685\n",
      "350 loss: 0.6295700599165523\n",
      "400 loss: 0.5747817176229814\n",
      "450 loss: 0.5723962573444142\n",
      "500 loss: 0.5999254654435551\n",
      "550 loss: 0.5256882274852079\n",
      "600 loss: 0.5597526290837456\n",
      "650 loss: 0.5097463832182043\n",
      "700 loss: 0.4902721687274821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 21:17:43,374] Trial 16 finished with value: 0.5366001209921354 and parameters: {'epochs': 716, 'batch_size': 32, 'learning_rate': 0.0003792981700233395, 'dropout': 0.42290929838069763, 'num_layers': 2, 'hidden_size': 16}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3670074434103731 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6937740760691026\n",
      "50 loss: 0.6866628177025739\n",
      "100 loss: 0.6870035949875327\n",
      "150 loss: 0.6868965555639828\n",
      "200 loss: 0.6868415229460773\n",
      "250 loss: 0.6868119415114907\n",
      "300 loss: 0.6867936253547668\n",
      "350 loss: 0.6867867182282841\n",
      "400 loss: 0.6867779668639687\n",
      "450 loss: 0.6867760384784025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 21:41:43,289] Trial 17 finished with value: 0.5 and parameters: {'epochs': 454, 'batch_size': 32, 'learning_rate': 0.0019063025847490555, 'dropout': 0.3670074434103731, 'num_layers': 1, 'hidden_size': 44}. Best is trial 11 with value: 0.5632183908045977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.7090201841460334\n",
      "50 loss: 0.6677167216936747\n",
      "100 loss: 0.6580239997969733\n",
      "150 loss: 0.6299784547752805\n",
      "200 loss: 0.5851173301537832\n",
      "250 loss: 0.506928465432591\n",
      "300 loss: 0.41920789227717453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 21:45:09,465] Trial 18 finished with value: 0.4773139745916516 and parameters: {'epochs': 326, 'batch_size': 64, 'learning_rate': 0.00017131066789410852, 'dropout': 0.4344952145656601, 'num_layers': 2, 'hidden_size': 71}. Best is trial 11 with value: 0.5632183908045977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6951716857797959\n",
      "50 loss: 0.6761466264724731\n",
      "100 loss: 0.6822971421129563\n",
      "150 loss: 0.6643088670337901\n",
      "200 loss: 0.6676151507041034\n",
      "250 loss: 0.6553929132573745\n",
      "300 loss: 0.6611413955688477\n",
      "350 loss: 0.6469814882558935\n",
      "400 loss: 0.6147902958533343\n",
      "450 loss: 0.6104859885047463\n",
      "500 loss: 0.5995604395866394\n",
      "550 loss: 0.6129781375913059\n",
      "600 loss: 0.647794164278928\n",
      "650 loss: 0.6469045894987443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 21:51:56,615] Trial 19 finished with value: 0.483666061705989 and parameters: {'epochs': 696, 'batch_size': 32, 'learning_rate': 0.00047388065088972703, 'dropout': 0.3880777442041123, 'num_layers': 2, 'hidden_size': 19}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.26395848677571154 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6927115847082699\n",
      "50 loss: 0.645237196894253\n",
      "100 loss: 0.6588604590472054\n",
      "150 loss: 0.635602099054\n",
      "200 loss: 0.4965985084281248\n",
      "250 loss: 0.558690327055314\n",
      "300 loss: 0.4993046303005779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 21:59:48,470] Trial 20 finished with value: 0.5287356321839081 and parameters: {'epochs': 349, 'batch_size': 32, 'learning_rate': 0.0002736715777978131, 'dropout': 0.26395848677571154, 'num_layers': 1, 'hidden_size': 114}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4593024489054125 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6901105677380281\n",
      "50 loss: 0.6272132536944222\n",
      "100 loss: 0.5335120436023263\n",
      "150 loss: 0.544363302343032\n",
      "200 loss: 0.35475589466445584\n",
      "250 loss: 0.32293157454799204\n",
      "300 loss: 0.16917317360639572\n",
      "350 loss: 0.08811518412959926\n",
      "400 loss: 0.04986647415139219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:06:31,229] Trial 21 finished with value: 0.5275257108287961 and parameters: {'epochs': 447, 'batch_size': 32, 'learning_rate': 0.00013215713354552627, 'dropout': 0.4593024489054125, 'num_layers': 1, 'hidden_size': 83}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4624069720951899 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6896760393591488\n",
      "50 loss: 0.6550521114293266\n",
      "100 loss: 0.5880048871040344\n",
      "150 loss: 0.4648723838960423\n",
      "200 loss: 0.38499299261499853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:10:00,986] Trial 22 finished with value: 0.4830611010284332 and parameters: {'epochs': 233, 'batch_size': 32, 'learning_rate': 0.0001037248566683432, 'dropout': 0.4624069720951899, 'num_layers': 1, 'hidden_size': 83}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.49844696563295715 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6906711739652297\n",
      "50 loss: 0.6337947600028094\n",
      "100 loss: 0.6195256114006042\n",
      "150 loss: 0.4888983140973484\n",
      "200 loss: 0.37709394710905414\n",
      "250 loss: 0.258665566716124\n",
      "300 loss: 0.13648984619580648\n",
      "350 loss: 0.0620043260483619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:13:17,011] Trial 23 finished with value: 0.5353901996370236 and parameters: {'epochs': 366, 'batch_size': 32, 'learning_rate': 0.00010111317070071278, 'dropout': 0.49844696563295715, 'num_layers': 1, 'hidden_size': 46}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.39673691669708244 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6872928878840279\n",
      "50 loss: 0.6042831627761617\n",
      "100 loss: 0.595218886347378\n",
      "150 loss: 0.5492377666866078\n",
      "200 loss: 0.48546259017551646\n",
      "250 loss: 0.44723696831394644\n",
      "300 loss: 0.3406454184476067\n",
      "350 loss: 0.2924247533082962\n",
      "400 loss: 0.2586020470103797\n",
      "450 loss: 0.1627135993912816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:19:10,883] Trial 24 finished with value: 0.49213551119177257 and parameters: {'epochs': 471, 'batch_size': 32, 'learning_rate': 0.00015746885064432193, 'dropout': 0.39673691669708244, 'num_layers': 1, 'hidden_size': 68}. Best is trial 11 with value: 0.5632183908045977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.718116925822364\n",
      "50 loss: 0.6740949418809679\n",
      "100 loss: 0.6547663278049893\n",
      "150 loss: 0.6467858950297037\n",
      "200 loss: 0.6159358554416232\n",
      "250 loss: 0.5607599947187636\n",
      "300 loss: 0.5607217649618784\n",
      "350 loss: 0.48784349858760834\n",
      "400 loss: 0.46624380018975997\n",
      "450 loss: 0.3520703338500526\n",
      "500 loss: 0.3664688484536277\n",
      "550 loss: 0.29865331533882356\n",
      "600 loss: 0.2957220690117942\n",
      "650 loss: 0.23924696129850215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:23:32,895] Trial 25 finished with value: 0.5375075620084695 and parameters: {'epochs': 665, 'batch_size': 64, 'learning_rate': 0.00025707968624189703, 'dropout': 0.4484694665729519, 'num_layers': 2, 'hidden_size': 39}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.40994231949432264 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.689884389148039\n",
      "50 loss: 0.6246776230194989\n",
      "100 loss: 0.5989847060512093\n",
      "150 loss: 0.5386473466368282\n",
      "200 loss: 0.5016019431983724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:27:16,169] Trial 26 finished with value: 0.4748941318814277 and parameters: {'epochs': 203, 'batch_size': 32, 'learning_rate': 0.0001475210574827274, 'dropout': 0.40994231949432264, 'num_layers': 1, 'hidden_size': 106}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4376962687546113 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.688681143171647\n",
      "50 loss: 0.6580625632229973\n",
      "100 loss: 0.6000668055870954\n",
      "150 loss: 0.5148740095250747\n",
      "200 loss: 0.43290607105283174\n",
      "250 loss: 0.43900297516409087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:31:41,237] Trial 27 finished with value: 0.5187537810042347 and parameters: {'epochs': 284, 'batch_size': 32, 'learning_rate': 0.00019499522923274398, 'dropout': 0.4376962687546113, 'num_layers': 1, 'hidden_size': 87}. Best is trial 11 with value: 0.5632183908045977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6897993438384112\n",
      "50 loss: 0.6784439788145178\n",
      "100 loss: 0.6679886369144216\n",
      "150 loss: 0.6643544926362879\n",
      "200 loss: 0.6741300821304321\n",
      "250 loss: 0.6493343956330243\n",
      "300 loss: 0.6364899488056407\n",
      "350 loss: 0.6148861664183\n",
      "400 loss: 0.5992918926126817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:41:18,839] Trial 28 finished with value: 0.5275257108287961 and parameters: {'epochs': 403, 'batch_size': 32, 'learning_rate': 0.0003292784551363751, 'dropout': 0.4781870466780192, 'num_layers': 2, 'hidden_size': 128}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3394461186770269 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6907182269626193\n",
      "50 loss: 0.6039873990747664\n",
      "100 loss: 0.5730424258444045\n",
      "150 loss: 0.36169621762302184\n",
      "200 loss: 0.26192284954918754\n",
      "250 loss: 0.11617313666890065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:43:41,977] Trial 29 finished with value: 0.5372050816696915 and parameters: {'epochs': 278, 'batch_size': 64, 'learning_rate': 0.0001279171730392705, 'dropout': 0.3394461186770269, 'num_layers': 1, 'hidden_size': 59}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4055725896501003 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6901808696634629\n",
      "50 loss: 0.6492583260816687\n",
      "100 loss: 0.6278803734218373\n",
      "150 loss: 0.5851711262674892\n",
      "200 loss: 0.5749865682686076\n",
      "250 loss: 0.5832088186460382\n",
      "300 loss: 0.5799786956871257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:51:14,846] Trial 30 finished with value: 0.5272232304900182 and parameters: {'epochs': 331, 'batch_size': 32, 'learning_rate': 0.0005594284176251418, 'dropout': 0.4055725896501003, 'num_layers': 1, 'hidden_size': 67}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.29212532789082774 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.692910885810852\n",
      "50 loss: 0.5576590538024903\n",
      "100 loss: 0.4156665295362473\n",
      "150 loss: 0.28647173196077347\n",
      "200 loss: 0.18315357752144337\n",
      "250 loss: 0.11614785520359874\n",
      "300 loss: 0.06206944352015853\n",
      "350 loss: 0.03564577726647258\n",
      "400 loss: 0.021521223871968687\n",
      "450 loss: 0.013695596379693597\n",
      "500 loss: 0.009182972129201517\n",
      "550 loss: 0.00641283456934616\n",
      "600 loss: 0.004634599448763765\n",
      "650 loss: 0.003442767412343528\n",
      "700 loss: 0.0026153568105655722\n",
      "750 loss: 0.002023596879735123\n",
      "800 loss: 0.001589580823929282\n",
      "850 loss: 0.0012645366812648717\n",
      "900 loss: 0.0010166623305849498\n",
      "950 loss: 0.0008244616179581499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:55:36,112] Trial 31 finished with value: 0.5538415003024802 and parameters: {'epochs': 970, 'batch_size': 128, 'learning_rate': 0.00021435712160336397, 'dropout': 0.29212532789082774, 'num_layers': 1, 'hidden_size': 30}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2813051653599367 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.7039988398551941\n",
      "50 loss: 0.5774086594581604\n",
      "100 loss: 0.43543277084827425\n",
      "150 loss: 0.2968498557806015\n",
      "200 loss: 0.18850649371743203\n",
      "250 loss: 0.10154827088117599\n",
      "300 loss: 0.054167489893734454\n",
      "350 loss: 0.03017286825925112\n",
      "400 loss: 0.0180264834954869\n",
      "450 loss: 0.011378705903189257\n",
      "500 loss: 0.007629395576077514\n",
      "550 loss: 0.005342033118358814\n",
      "600 loss: 0.0038724942787666805\n",
      "650 loss: 0.0028863417683169246\n",
      "700 loss: 0.0021999911958118902\n",
      "750 loss: 0.001706693384767277\n",
      "800 loss: 0.0013432199531962397\n",
      "850 loss: 0.0010696222430851775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 22:59:50,090] Trial 32 finished with value: 0.5532365396249244 and parameters: {'epochs': 862, 'batch_size': 128, 'learning_rate': 0.00021114646184647255, 'dropout': 0.2813051653599367, 'num_layers': 1, 'hidden_size': 33}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3729321258723265 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6826502203941345\n",
      "50 loss: 0.5999642431735992\n",
      "100 loss: 0.4942696332931519\n",
      "150 loss: 0.43589219748973845\n",
      "200 loss: 0.27405172362923624\n",
      "250 loss: 0.20236274115741254\n",
      "300 loss: 0.16267302073538303\n",
      "350 loss: 0.11597763355821371\n",
      "400 loss: 0.07095976285636425\n",
      "450 loss: 0.048570477915927765\n",
      "500 loss: 0.0333117603790015\n",
      "550 loss: 0.02317349079530686\n",
      "600 loss: 0.016608382319100203\n",
      "650 loss: 0.012098344648256898\n",
      "700 loss: 0.009024702163878828\n",
      "750 loss: 0.006838671700097621\n",
      "800 loss: 0.005255640082759782\n",
      "850 loss: 0.0041014436254045\n",
      "900 loss: 0.003228340548230335\n",
      "950 loss: 0.002565922314533964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 23:04:05,430] Trial 33 finished with value: 0.5366001209921354 and parameters: {'epochs': 978, 'batch_size': 128, 'learning_rate': 0.00014682762384336047, 'dropout': 0.3729321258723265, 'num_layers': 1, 'hidden_size': 27}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.28120134120769835 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.68904709815979\n",
      "50 loss: 0.5529294580221176\n",
      "100 loss: 0.47739591002464293\n",
      "150 loss: 0.6155458196997643\n",
      "200 loss: 0.34766973927617073\n",
      "250 loss: 0.323787391372025\n",
      "300 loss: 0.2384811094030738\n",
      "350 loss: 0.2145755714736879\n",
      "400 loss: 0.18634062726050615\n",
      "450 loss: 0.1250591149320826\n",
      "500 loss: 0.1939862946048379\n",
      "550 loss: 0.18183635715395213\n",
      "600 loss: 0.5615431308746338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 12:39:16,306] Trial 34 finished with value: 0.5447670901391409 and parameters: {'epochs': 628, 'batch_size': 128, 'learning_rate': 0.0008491363756761092, 'dropout': 0.28120134120769835, 'num_layers': 1, 'hidden_size': 78}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3429776001343554 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6844919919967651\n",
      "50 loss: 0.5060008645057679\n",
      "100 loss: 0.35205146446824076\n",
      "150 loss: 0.2627152010798454\n",
      "200 loss: 0.08175552021712065\n",
      "250 loss: 0.030963471229188144\n",
      "300 loss: 0.01477274247445166\n",
      "350 loss: 0.008356819726759567\n",
      "400 loss: 0.005086047836812213\n",
      "450 loss: 0.003416827370529063\n",
      "500 loss: 0.0023395371114020235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 12:44:52,743] Trial 35 finished with value: 0.5208711433756806 and parameters: {'epochs': 511, 'batch_size': 128, 'learning_rate': 0.0002597626877009149, 'dropout': 0.3429776001343554, 'num_layers': 1, 'hidden_size': 58}. Best is trial 11 with value: 0.5632183908045977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6928933461507162\n",
      "50 loss: 0.682231154706743\n",
      "100 loss: 0.6756209598647224\n",
      "150 loss: 0.6787914964887831\n",
      "200 loss: 0.6858069962925382\n",
      "250 loss: 0.6857696771621704\n",
      "300 loss: 0.6857712533738878\n",
      "350 loss: 0.6857742203606499\n",
      "400 loss: 0.6857728494538201\n",
      "450 loss: 0.6857537163628472\n",
      "500 loss: 0.6857593523131477\n",
      "550 loss: 0.6857604318194919\n",
      "600 loss: 0.6857604450649686\n",
      "650 loss: 0.6857604251967536\n",
      "700 loss: 0.6857604251967536\n",
      "750 loss: 0.6857694917254977\n",
      "800 loss: 0.6857604649331834\n",
      "850 loss: 0.6857604185740153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 15:49:49,120] Trial 36 finished with value: 0.5 and parameters: {'epochs': 854, 'batch_size': 64, 'learning_rate': 0.0019010952788273705, 'dropout': 0.3339536065119777, 'num_layers': 2, 'hidden_size': 182}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.293314991032699 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6948380926076103\n",
      "50 loss: 0.6715488784453448\n",
      "100 loss: 0.6052741829086753\n",
      "150 loss: 0.47487758801263924\n",
      "200 loss: 0.44940075716551614\n",
      "250 loss: 0.3405766969217974\n",
      "300 loss: 0.3257558440022609\n",
      "350 loss: 0.2575289193979081\n",
      "400 loss: 0.23230199614430175\n",
      "450 loss: 0.25778996736249504\n",
      "500 loss: 0.39607160669915814\n",
      "550 loss: 0.1912534396876307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 15:56:02,974] Trial 37 finished with value: 0.5269207501512401 and parameters: {'epochs': 575, 'batch_size': 32, 'learning_rate': 0.00018580487038618757, 'dropout': 0.293314991032699, 'num_layers': 1, 'hidden_size': 34}. Best is trial 11 with value: 0.5632183908045977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6878166317939758\n",
      "50 loss: 0.628340208530426\n",
      "100 loss: 0.48369427770376205\n",
      "150 loss: 0.21802146015688778\n",
      "200 loss: 0.03235671471338719\n",
      "250 loss: 0.005738263198873028\n",
      "300 loss: 0.0017102576704928652\n",
      "350 loss: 0.001241828647107468\n",
      "400 loss: 0.03975966565776616\n",
      "450 loss: 0.0069860029470874\n",
      "500 loss: 0.0031704205845016985\n",
      "550 loss: 0.006803918691002764\n",
      "600 loss: 0.0013331608468433842\n",
      "650 loss: 0.00278662471100688\n",
      "700 loss: 0.04620659914216958\n",
      "750 loss: 0.0017443268770875875\n",
      "800 loss: 0.002602851723713684\n",
      "850 loss: 0.0015150942803302315\n",
      "900 loss: 0.00145600966825441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 16:20:48,213] Trial 38 finished with value: 0.46854204476709016 and parameters: {'epochs': 918, 'batch_size': 128, 'learning_rate': 0.00011868931105205479, 'dropout': 0.23859947459350583, 'num_layers': 2, 'hidden_size': 161}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4277755148431085 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6914249798830818\n",
      "50 loss: 0.6412253379821777\n",
      "100 loss: 0.5921461950330174\n",
      "150 loss: 0.5238395946867326\n",
      "200 loss: 0.4910583539920695\n",
      "250 loss: 0.4716906643965665\n",
      "300 loss: 0.39321375243804035\n",
      "350 loss: 0.3474428864086376\n",
      "400 loss: 0.35184386394479694\n",
      "450 loss: 0.434521071941537\n",
      "500 loss: 0.1948905687998323\n",
      "550 loss: 0.2174539383062545\n",
      "600 loss: 0.3349649577456362\n",
      "650 loss: 0.1249931598739589\n",
      "700 loss: 0.11923848752699354\n",
      "750 loss: 0.8732016687007511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 16:26:16,653] Trial 39 finished with value: 0.5450695704779189 and parameters: {'epochs': 783, 'batch_size': 32, 'learning_rate': 0.00022873072347648585, 'dropout': 0.4277755148431085, 'num_layers': 1, 'hidden_size': 16}. Best is trial 11 with value: 0.5632183908045977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.7072600960731507\n",
      "50 loss: 0.6687234282493592\n",
      "100 loss: 0.6858807682991028\n",
      "150 loss: 0.5758341550827026\n",
      "200 loss: 0.5592415153980255\n",
      "250 loss: 0.49221269860863687\n",
      "300 loss: 0.5043917506933212\n",
      "350 loss: 0.42156357914209364\n",
      "400 loss: 0.40858154743909836\n",
      "450 loss: 0.4274469405412674\n",
      "500 loss: 0.39253230914473536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 16:32:29,393] Trial 40 finished with value: 0.49939503932244406 and parameters: {'epochs': 527, 'batch_size': 128, 'learning_rate': 0.00035130420206505976, 'dropout': 0.37967189690571135, 'num_layers': 3, 'hidden_size': 56}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2572650314754375 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.7115166425704956\n",
      "50 loss: 0.605391901731491\n",
      "100 loss: 0.46887979209423064\n",
      "150 loss: 0.3272637888789177\n",
      "200 loss: 0.20102753564715387\n",
      "250 loss: 0.18656204771250487\n",
      "300 loss: 0.08962815152481199\n",
      "350 loss: 0.050479713408276436\n",
      "400 loss: 0.030536329187452794\n",
      "450 loss: 0.019171841652132572\n",
      "500 loss: 0.012661749788094312\n",
      "550 loss: 0.008784203068353235\n",
      "600 loss: 0.0063342194189317524\n",
      "650 loss: 0.004689065931597724\n",
      "700 loss: 0.00354619060526602\n",
      "750 loss: 0.0027363227200112306\n",
      "800 loss: 0.0021456616013892926\n",
      "850 loss: 0.0016957458996330389\n",
      "900 loss: 0.0013582162115199025\n",
      "950 loss: 0.0010993253446940799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 16:39:18,232] Trial 41 finished with value: 0.47610405323653965 and parameters: {'epochs': 1000, 'batch_size': 128, 'learning_rate': 0.00020594035765670376, 'dropout': 0.2572650314754375, 'num_layers': 1, 'hidden_size': 31}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3019870984755858 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6859615445137024\n",
      "50 loss: 0.5265722692012786\n",
      "100 loss: 0.38006359338760376\n",
      "150 loss: 0.27550646513700483\n",
      "200 loss: 0.1769096814095974\n",
      "250 loss: 0.07232378842309117\n",
      "300 loss: 0.03609889685176313\n",
      "350 loss: 0.01938319255132228\n",
      "400 loss: 0.01135177348041907\n",
      "450 loss: 0.007136256445664913\n",
      "500 loss: 0.004748435277724639\n",
      "550 loss: 0.0032008620823035018\n",
      "600 loss: 0.0022578345000511036\n",
      "650 loss: 0.0016557726237806491\n",
      "700 loss: 0.0012642288391361944\n",
      "750 loss: 0.000942175360978581\n",
      "800 loss: 0.0007242764077091124\n",
      "850 loss: 0.0005632461507047992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 16:46:28,048] Trial 42 finished with value: 0.5468844525105868 and parameters: {'epochs': 886, 'batch_size': 128, 'learning_rate': 0.00016670315867631818, 'dropout': 0.3019870984755858, 'num_layers': 1, 'hidden_size': 40}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.32390557019515964 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6941353917121887\n",
      "50 loss: 0.6055372297763825\n",
      "100 loss: 0.5154565215110779\n",
      "150 loss: 0.40885656177997587\n",
      "200 loss: 0.3154697559773922\n",
      "250 loss: 0.26323597878217697\n",
      "300 loss: 0.16168734710663557\n",
      "350 loss: 0.10992817729711532\n",
      "400 loss: 0.07364564826712013\n",
      "450 loss: 0.06475530336610973\n",
      "500 loss: 0.09985195072367788\n",
      "550 loss: 0.04007650043349713\n",
      "600 loss: 0.05847192917717621\n",
      "650 loss: 0.01893985370406881\n",
      "700 loss: 0.014468465780373663\n",
      "750 loss: 0.011342472571413964\n",
      "800 loss: 0.5662337511777877\n",
      "850 loss: 0.023915082029998303\n",
      "900 loss: 0.013434203015640378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 16:52:54,245] Trial 43 finished with value: 0.5459770114942528 and parameters: {'epochs': 945, 'batch_size': 128, 'learning_rate': 0.00012568867065306118, 'dropout': 0.32390557019515964, 'num_layers': 1, 'hidden_size': 29}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2672717346021501 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6886305809020996\n",
      "50 loss: 0.5408712685108185\n",
      "100 loss: 0.370945829898119\n",
      "150 loss: 0.4723798468708992\n",
      "200 loss: 0.15352763291448354\n",
      "250 loss: 0.1310569044202566\n",
      "300 loss: 0.06159224743023515\n",
      "350 loss: 0.24429734330624342\n",
      "400 loss: 0.1593709741719067\n",
      "450 loss: 0.058858638256788255\n",
      "500 loss: 0.0333014105213806\n",
      "550 loss: 0.020072753017302603\n",
      "600 loss: 0.012845928285969421\n",
      "650 loss: 0.008694252325221896\n",
      "700 loss: 0.006123781009227969\n",
      "750 loss: 0.004451392289774958\n",
      "800 loss: 0.003317487311142031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 17:12:14,116] Trial 44 finished with value: 0.5626134301270417 and parameters: {'epochs': 819, 'batch_size': 128, 'learning_rate': 0.0005070568366321074, 'dropout': 0.2672717346021501, 'num_layers': 1, 'hidden_size': 47}. Best is trial 11 with value: 0.5632183908045977.\n",
      "c:\\Users\\guilherme_trintinali\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.35678676211656435 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6874679446220398\n",
      "50 loss: 0.5020843863487243\n",
      "100 loss: 0.3668241139501333\n",
      "150 loss: 0.37223603539168837\n",
      "200 loss: 0.18233772059902548\n",
      "250 loss: 0.10169820711016656\n",
      "300 loss: 0.08439428987912834\n",
      "350 loss: 0.11551592815667391\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dir = os.path.join('..', 'data', 'processed')\n",
    "\n",
    "features_name = 'daily_features.pkl'\n",
    "input_dir_features = os.path.join('..', 'data', 'features')\n",
    "\n",
    "df_daily = pd.read_parquet(os.path.join(input_dir, 'df_daily.parquet')).replace(\"/\", \"\\\\\")\n",
    "df2 = pd.read_parquet(os.path.join(input_dir, 'df_timestamp.parquet')).replace(\"/\", \"\\\\\")\n",
    "daily_features = joblib.load(os.path.join(input_dir_features, 'daily_features.pkl'))\n",
    "timnestamp_features = joblib.load(os.path.join(input_dir_features, '15min_timestamp_features.pkl'))\n",
    "\n",
    "models = ['LSTM', 'GRU', 'CNN-LSTM', 'CNN-GRU']\n",
    "features = daily_features\n",
    "targets = ['behavior_target', 'open_price_target', 'close_price_target']\n",
    "sliding_windows = [7, 15, 30, 45, 60] \n",
    "look_forwards = [1]  \n",
    "\n",
    "# Call the optimize_models function\n",
    "study_results = optimize_models(df=df_daily,\n",
    "                                targets=targets,\n",
    "                                features=features, \n",
    "                                look_backs=sliding_windows,\n",
    "                                look_forwards=look_forwards,\n",
    "                                target_dir='daily', \n",
    "                                max_samples=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = os.path.join('..', 'data', 'processed')\n",
    "\n",
    "features_name = 'daily_features.pkl'\n",
    "input_dir_features = os.path.join('..', 'data', 'features')\n",
    "\n",
    "df_daily = pd.read_parquet(os.path.join(input_dir, 'df_daily.parquet')).replace(\"/\", \"\\\\\")\n",
    "df2 = pd.read_parquet(os.path.join(input_dir, 'df_timestamp.parquet')).replace(\"/\", \"\\\\\")\n",
    "daily_features = joblib.load(os.path.join(input_dir_features, 'daily_features.pkl'))\n",
    "timnestamp_features = joblib.load(os.path.join(input_dir_features, '15min_timestamp_features.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = daily_features[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feature for feature in daily_features if 'BGI$' in feature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # True se disponível\n",
    "\n",
    "# Configura o dispositivo para GPU se disponível\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  # \"cuda\" ou \"cpu\"\n",
    "\n",
    "# Opcional: Verifica qual GPU está sendo usada\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_daily = pd.read_parquet(os.path.join(input_dir, 'df_daily.parquet')).replace(\"/\", \"\\\\\")\n",
    "\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily.behavior_target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily, _ = scale_features(df_daily) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_df, teste_df = segment_data(df_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, y_dates = data_to_array(treino_df.copy(), \n",
    "                                                          window_size=30,\n",
    "                                                          target='behavior_target', \n",
    "                                                          features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, y_dates = data_to_array(teste_df.copy(), \n",
    "                                                          window_size=30,\n",
    "                                                          target='behavior_target', \n",
    "                                                          features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long if 'behavior' in targets else torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(504, 64, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/5000, Loss: 0.6911\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.4732558139534884\n",
      "0.5\n",
      "0.5011627906976744\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.4866279069767442\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.4593023255813954\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5180232558139534\n",
      "0.5\n",
      "0.43081395348837204\n",
      "0.46395348837209305\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5180232558139534\n",
      "0.4377906976744187\n",
      "0.5\n",
      "0.5180232558139534\n",
      "0.5331395348837209\n",
      "0.5232558139534884\n",
      "0.5261627906976745\n",
      "0.5412790697674419\n",
      "0.5331395348837209\n",
      "0.5412790697674419\n",
      "0.5180232558139534\n",
      "0.4947674418604651\n",
      "0.5034883720930232\n",
      "0.5\n",
      "0.5\n",
      "0.5232558139534884\n",
      "0.5098837209302325\n",
      "0.5215116279069767\n",
      "0.5308139534883721\n",
      "0.5232558139534884\n",
      "0.44593023255813946\n",
      "0.5046511627906977\n",
      "0.5837209302325582\n",
      "0.5395348837209303\n",
      "0.5511627906976745\n",
      "0.5459302325581394\n",
      "0.5476744186046512\n",
      "0.5174418604651163\n",
      "0.5313953488372093\n",
      "0.5093023255813953\n",
      "0.502906976744186\n",
      "0.5877906976744186\n",
      "0.5313953488372093\n",
      "0.5796511627906976\n",
      "0.5959302325581395\n",
      "0.561046511627907\n",
      "0.5755813953488372\n",
      "0.5674418604651162\n",
      "0.6040697674418606\n",
      "0.5529069767441861\n",
      "0.547093023255814\n",
      "0.5645348837209302\n",
      "0.5593023255813954\n",
      "0.5994186046511628\n",
      "0.6238372093023257\n",
      "0.5075581395348837\n",
      "0.561046511627907\n",
      "0.5290697674418605\n",
      "0.5145348837209303\n",
      "0.5331395348837209\n",
      "0.611046511627907\n",
      "0.5261627906976745\n",
      "0.5709302325581396\n",
      "0.6093023255813954\n",
      "0.5761627906976744\n",
      "0.536046511627907\n",
      "0.5377906976744187\n",
      "0.5226744186046511\n",
      "0.5412790697674419\n",
      "0.538953488372093\n",
      "0.5593023255813954\n",
      "0.5040697674418605\n",
      "0.5058139534883721\n",
      "0.6075581395348837\n",
      "0.5325581395348837\n",
      "0.5529069767441861\n",
      "0.5395348837209303\n",
      "0.5395348837209303\n",
      "Epoch 201/5000, Loss: 0.5221\n",
      "0.5511627906976745\n",
      "0.5744186046511628\n",
      "0.5744186046511628\n",
      "0.5412790697674419\n",
      "0.6290697674418604\n",
      "0.6040697674418606\n",
      "0.5627906976744187\n",
      "0.5395348837209303\n",
      "0.5511627906976745\n",
      "0.5627906976744187\n",
      "0.561046511627907\n",
      "0.5145348837209303\n",
      "0.5924418604651163\n",
      "0.5627906976744187\n",
      "0.5575581395348838\n",
      "0.502906976744186\n",
      "0.5511627906976745\n",
      "0.5779069767441861\n",
      "0.5627906976744187\n",
      "0.5412790697674419\n",
      "0.5377906976744187\n",
      "0.5761627906976744\n",
      "0.586046511627907\n",
      "0.5627906976744187\n",
      "0.5255813953488372\n",
      "0.5941860465116279\n",
      "0.5244186046511627\n",
      "0.5627906976744187\n",
      "0.5662790697674419\n",
      "0.5279069767441861\n",
      "0.5825581395348837\n",
      "0.5476744186046512\n",
      "0.49767441860465117\n",
      "0.5761627906976744\n",
      "0.561046511627907\n",
      "0.5296511627906977\n",
      "0.5877906976744186\n",
      "0.4994186046511628\n",
      "0.536046511627907\n",
      "0.6226744186046512\n",
      "0.5459302325581394\n",
      "0.5627906976744187\n",
      "0.5726744186046512\n",
      "0.6325581395348837\n",
      "0.5162790697674419\n",
      "0.561046511627907\n",
      "0.5476744186046512\n",
      "0.5296511627906977\n",
      "0.5726744186046512\n",
      "0.5976744186046512\n",
      "0.5843023255813954\n",
      "0.586046511627907\n",
      "0.5395348837209303\n",
      "0.5441860465116279\n",
      "0.5877906976744186\n",
      "0.5575581395348838\n",
      "0.5709302325581396\n",
      "0.5593023255813954\n",
      "0.5941860465116279\n",
      "0.5976744186046512\n",
      "0.5924418604651163\n",
      "0.5976744186046512\n",
      "0.5593023255813954\n",
      "0.5761627906976744\n",
      "0.5994186046511628\n",
      "0.5645348837209302\n",
      "0.5494186046511628\n",
      "0.6011627906976744\n",
      "0.5377906976744187\n",
      "0.6191860465116279\n",
      "0.611046511627907\n",
      "0.586046511627907\n",
      "0.586046511627907\n",
      "0.586046511627907\n",
      "0.6308139534883721\n",
      "0.5843023255813954\n",
      "0.5476744186046512\n",
      "0.6191860465116279\n",
      "0.5145348837209303\n",
      "0.611046511627907\n",
      "0.5226744186046511\n",
      "0.5674418604651162\n",
      "0.6325581395348837\n",
      "0.5924418604651163\n",
      "0.586046511627907\n",
      "0.6441860465116279\n",
      "0.5761627906976744\n",
      "0.5959302325581395\n",
      "0.5511627906976745\n",
      "0.5825581395348837\n",
      "0.5529069767441861\n",
      "0.5511627906976745\n",
      "0.611046511627907\n",
      "0.6325581395348837\n",
      "0.5877906976744186\n",
      "0.6226744186046512\n",
      "0.5976744186046512\n",
      "0.5627906976744187\n",
      "0.6459302325581394\n",
      "0.5976744186046512\n",
      "0.5976744186046512\n",
      "0.611046511627907\n",
      "0.6226744186046512\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6441860465116279\n",
      "0.586046511627907\n",
      "0.5744186046511628\n",
      "0.6209302325581395\n",
      "0.5976744186046512\n",
      "0.6191860465116279\n",
      "0.5430232558139534\n",
      "0.5959302325581395\n",
      "0.5691860465116279\n",
      "0.6308139534883721\n",
      "0.5994186046511628\n",
      "0.5593023255813954\n",
      "0.5511627906976745\n",
      "0.6011627906976744\n",
      "0.6093023255813954\n",
      "0.6459302325581394\n",
      "0.6058139534883721\n",
      "0.5976744186046512\n",
      "0.6191860465116279\n",
      "0.5959302325581395\n",
      "0.5994186046511628\n",
      "0.5744186046511628\n",
      "0.6191860465116279\n",
      "0.5877906976744186\n",
      "0.6575581395348836\n",
      "0.6290697674418604\n",
      "0.5645348837209302\n",
      "0.6424418604651163\n",
      "0.5744186046511628\n",
      "0.586046511627907\n",
      "0.5994186046511628\n",
      "0.5976744186046512\n",
      "0.6075581395348837\n",
      "0.6093023255813954\n",
      "0.5994186046511628\n",
      "0.6075581395348837\n",
      "0.6226744186046512\n",
      "0.5959302325581395\n",
      "0.5843023255813954\n",
      "0.5726744186046512\n",
      "0.611046511627907\n",
      "0.6308139534883721\n",
      "0.5627906976744187\n",
      "0.6540697674418605\n",
      "0.6093023255813954\n",
      "0.6191860465116279\n",
      "0.6325581395348837\n",
      "0.5843023255813954\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.6093023255813954\n",
      "0.6191860465116279\n",
      "0.586046511627907\n",
      "0.5924418604651163\n",
      "0.5744186046511628\n",
      "0.6075581395348837\n",
      "0.5244186046511627\n",
      "0.536046511627907\n",
      "0.5726744186046512\n",
      "0.5377906976744187\n",
      "0.5843023255813954\n",
      "0.586046511627907\n",
      "0.5825581395348837\n",
      "0.5546511627906977\n",
      "0.702325581395349\n",
      "0.6209302325581395\n",
      "0.5627906976744187\n",
      "0.5825581395348837\n",
      "0.5511627906976745\n",
      "0.5843023255813954\n",
      "0.5162790697674419\n",
      "0.6093023255813954\n",
      "0.5994186046511628\n",
      "0.5377906976744187\n",
      "0.5744186046511628\n",
      "0.6308139534883721\n",
      "0.5877906976744186\n",
      "0.6325581395348837\n",
      "0.5395348837209303\n",
      "0.5959302325581395\n",
      "0.502906976744186\n",
      "0.6308139534883721\n",
      "0.6011627906976744\n",
      "0.6308139534883721\n",
      "0.5627906976744187\n",
      "0.5825581395348837\n",
      "0.586046511627907\n",
      "0.586046511627907\n",
      "0.6558139534883721\n",
      "0.5877906976744186\n",
      "0.6093023255813954\n",
      "0.5511627906976745\n",
      "0.6209302325581395\n",
      "Epoch 401/5000, Loss: 0.1614\n",
      "0.5843023255813954\n",
      "0.5627906976744187\n",
      "0.5959302325581395\n",
      "0.5877906976744186\n",
      "0.6209302325581395\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.561046511627907\n",
      "0.6075581395348837\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.5976744186046512\n",
      "0.5843023255813954\n",
      "0.6290697674418604\n",
      "0.5744186046511628\n",
      "0.5691860465116279\n",
      "0.6244186046511628\n",
      "0.5825581395348837\n",
      "0.5994186046511628\n",
      "0.5825581395348837\n",
      "0.5994186046511628\n",
      "0.5959302325581395\n",
      "0.6575581395348836\n",
      "0.6325581395348837\n",
      "0.6075581395348837\n",
      "0.6209302325581395\n",
      "0.5976744186046512\n",
      "0.6558139534883721\n",
      "0.5877906976744186\n",
      "0.5744186046511628\n",
      "0.5744186046511628\n",
      "0.5976744186046512\n",
      "0.6325581395348837\n",
      "0.5744186046511628\n",
      "0.6093023255813954\n",
      "0.5994186046511628\n",
      "0.6127906976744187\n",
      "0.5843023255813954\n",
      "0.6244186046511628\n",
      "0.6593023255813953\n",
      "0.586046511627907\n",
      "0.5976744186046512\n",
      "0.6593023255813953\n",
      "0.5843023255813954\n",
      "0.6709302325581395\n",
      "0.6476744186046511\n",
      "0.5825581395348837\n",
      "0.611046511627907\n",
      "0.5744186046511628\n",
      "0.7075581395348837\n",
      "0.586046511627907\n",
      "0.6226744186046512\n",
      "0.6209302325581395\n",
      "0.6343023255813953\n",
      "0.6575581395348836\n",
      "0.6593023255813953\n",
      "0.6459302325581394\n",
      "0.6825581395348838\n",
      "0.6209302325581395\n",
      "0.6691860465116279\n",
      "0.6244186046511628\n",
      "0.5994186046511628\n",
      "0.5726744186046512\n",
      "0.6209302325581395\n",
      "0.5877906976744186\n",
      "0.6093023255813954\n",
      "0.586046511627907\n",
      "0.611046511627907\n",
      "0.5843023255813954\n",
      "0.6343023255813953\n",
      "0.611046511627907\n",
      "0.6127906976744187\n",
      "0.5726744186046512\n",
      "0.6343023255813953\n",
      "0.6459302325581394\n",
      "0.6575581395348836\n",
      "0.561046511627907\n",
      "0.6459302325581394\n",
      "0.6343023255813953\n",
      "0.586046511627907\n",
      "0.561046511627907\n",
      "0.5825581395348837\n",
      "0.6343023255813953\n",
      "0.5476744186046512\n",
      "0.5877906976744186\n",
      "0.5825581395348837\n",
      "0.5511627906976745\n",
      "0.6226744186046512\n",
      "0.6011627906976744\n",
      "0.6441860465116279\n",
      "0.5761627906976744\n",
      "0.561046511627907\n",
      "0.6790697674418606\n",
      "0.6709302325581395\n",
      "0.5843023255813954\n",
      "0.6459302325581394\n",
      "0.5459302325581394\n",
      "0.6226744186046512\n",
      "0.6191860465116279\n",
      "0.6058139534883721\n",
      "0.5093023255813953\n",
      "0.6343023255813953\n",
      "0.6441860465116279\n",
      "0.6093023255813954\n",
      "0.6441860465116279\n",
      "0.6209302325581395\n",
      "0.5709302325581396\n",
      "0.6343023255813953\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6825581395348838\n",
      "0.6941860465116279\n",
      "0.6459302325581394\n",
      "0.6593023255813953\n",
      "0.6459302325581394\n",
      "0.611046511627907\n",
      "0.6459302325581394\n",
      "0.6709302325581395\n",
      "0.6941860465116279\n",
      "0.6709302325581395\n",
      "0.6709302325581395\n",
      "0.6459302325581394\n",
      "0.6941860465116279\n",
      "0.6593023255813953\n",
      "0.7058139534883721\n",
      "0.6325581395348837\n",
      "0.6941860465116279\n",
      "0.6424418604651163\n",
      "0.6825581395348838\n",
      "0.6494186046511627\n",
      "0.6709302325581395\n",
      "0.611046511627907\n",
      "0.6325581395348837\n",
      "0.6843023255813954\n",
      "0.6558139534883721\n",
      "0.6494186046511627\n",
      "0.6459302325581394\n",
      "0.6941860465116279\n",
      "0.6843023255813954\n",
      "0.6825581395348838\n",
      "0.6575581395348836\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6075581395348837\n",
      "0.6191860465116279\n",
      "0.5959302325581395\n",
      "0.6441860465116279\n",
      "0.6093023255813954\n",
      "0.6226744186046512\n",
      "0.6325581395348837\n",
      "0.6476744186046511\n",
      "0.6325581395348837\n",
      "0.5709302325581396\n",
      "0.5843023255813954\n",
      "0.6808139534883721\n",
      "0.6558139534883721\n",
      "0.7058139534883721\n",
      "0.6674418604651163\n",
      "0.6558139534883721\n",
      "0.6244186046511628\n",
      "0.6308139534883721\n",
      "0.6209302325581395\n",
      "0.4877906976744187\n",
      "0.6575581395348836\n",
      "0.6709302325581395\n",
      "0.6709302325581395\n",
      "0.6459302325581394\n",
      "0.6709302325581395\n",
      "0.6459302325581394\n",
      "0.6209302325581395\n",
      "0.6825581395348838\n",
      "0.6459302325581394\n",
      "0.6575581395348836\n",
      "0.5843023255813954\n",
      "0.6209302325581395\n",
      "0.5994186046511628\n",
      "0.7058139534883721\n",
      "0.5476744186046512\n",
      "0.5994186046511628\n",
      "0.636046511627907\n",
      "0.5726744186046512\n",
      "0.6593023255813953\n",
      "0.6941860465116279\n",
      "0.6941860465116279\n",
      "0.6709302325581395\n",
      "0.6825581395348838\n",
      "0.6575581395348836\n",
      "0.636046511627907\n",
      "0.6209302325581395\n",
      "0.6575581395348836\n",
      "0.6941860465116279\n",
      "0.6691860465116279\n",
      "0.6709302325581395\n",
      "0.586046511627907\n",
      "0.6593023255813953\n",
      "0.636046511627907\n",
      "Epoch 601/5000, Loss: 0.0242\n",
      "0.6093023255813954\n",
      "0.6011627906976744\n",
      "0.6476744186046511\n",
      "0.6244186046511628\n",
      "0.6825581395348838\n",
      "0.6691860465116279\n",
      "0.6459302325581394\n",
      "0.6476744186046511\n",
      "0.6476744186046511\n",
      "0.6209302325581395\n",
      "0.6808139534883721\n",
      "0.6941860465116279\n",
      "0.6593023255813953\n",
      "0.6825581395348838\n",
      "0.7191860465116279\n",
      "0.6709302325581395\n",
      "0.6709302325581395\n",
      "0.6459302325581394\n",
      "0.6941860465116279\n",
      "0.6459302325581394\n",
      "0.5593023255813954\n",
      "0.6325581395348837\n",
      "0.6575581395348836\n",
      "0.5994186046511628\n",
      "0.6325581395348837\n",
      "0.5825581395348837\n",
      "0.6209302325581395\n",
      "0.611046511627907\n",
      "0.6593023255813953\n",
      "0.6343023255813953\n",
      "0.6709302325581395\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.5627906976744187\n",
      "0.6226744186046512\n",
      "0.7075581395348837\n",
      "0.6191860465116279\n",
      "0.6808139534883721\n",
      "0.6191860465116279\n",
      "0.6325581395348837\n",
      "0.6575581395348836\n",
      "0.6924418604651162\n",
      "0.6575581395348836\n",
      "0.6308139534883721\n",
      "0.6959302325581395\n",
      "0.7058139534883721\n",
      "0.6075581395348837\n",
      "0.6191860465116279\n",
      "0.6424418604651163\n",
      "0.6540697674418605\n",
      "0.5709302325581396\n",
      "0.6674418604651163\n",
      "0.6540697674418605\n",
      "0.6308139534883721\n",
      "0.5808139534883722\n",
      "0.6575581395348836\n",
      "0.5593023255813954\n",
      "0.6441860465116279\n",
      "0.7191860465116279\n",
      "0.6058139534883721\n",
      "0.6209302325581395\n",
      "0.6093023255813954\n",
      "0.6209302325581395\n",
      "0.6343023255813953\n",
      "0.6226744186046512\n",
      "0.6459302325581394\n",
      "0.6441860465116279\n",
      "0.6691860465116279\n",
      "0.6941860465116279\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6941860465116279\n",
      "0.6691860465116279\n",
      "0.6459302325581394\n",
      "0.6825581395348838\n",
      "0.6593023255813953\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6709302325581395\n",
      "0.6959302325581395\n",
      "0.6209302325581395\n",
      "0.6709302325581395\n",
      "0.6593023255813953\n",
      "0.6476744186046511\n",
      "0.6825581395348838\n",
      "0.6709302325581395\n",
      "0.6825581395348838\n",
      "0.6959302325581395\n",
      "0.6226744186046512\n",
      "0.6593023255813953\n",
      "0.6575581395348836\n",
      "0.6343023255813953\n",
      "0.6226744186046512\n",
      "0.6575581395348836\n",
      "0.6343023255813953\n",
      "0.6941860465116279\n",
      "0.6825581395348838\n",
      "0.6593023255813953\n",
      "0.6459302325581394\n",
      "0.6343023255813953\n",
      "0.6726744186046512\n",
      "0.6843023255813954\n",
      "0.6540697674418605\n",
      "0.7058139534883721\n",
      "0.6709302325581395\n",
      "0.6709302325581395\n",
      "0.6843023255813954\n",
      "0.6825581395348838\n",
      "0.6325581395348837\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6674418604651163\n",
      "0.636046511627907\n",
      "0.6476744186046511\n",
      "0.6691860465116279\n",
      "0.6593023255813953\n",
      "0.6459302325581394\n",
      "0.6843023255813954\n",
      "0.6476744186046511\n",
      "0.6459302325581394\n",
      "0.6691860465116279\n",
      "0.6343023255813953\n",
      "0.611046511627907\n",
      "0.6709302325581395\n",
      "0.6424418604651163\n",
      "0.6093023255813954\n",
      "0.6790697674418606\n",
      "0.6459302325581394\n",
      "0.6343023255813953\n",
      "0.6691860465116279\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6343023255813953\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6709302325581395\n",
      "0.6459302325581394\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6075581395348837\n",
      "0.6209302325581395\n",
      "0.6226744186046512\n",
      "0.6093023255813954\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6093023255813954\n",
      "0.6209302325581395\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6343023255813953\n",
      "0.6459302325581394\n",
      "0.6691860465116279\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6924418604651162\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6441860465116279\n",
      "0.6691860465116279\n",
      "0.6808139534883721\n",
      "0.7058139534883721\n",
      "0.6924418604651162\n",
      "0.6674418604651163\n",
      "0.6558139534883721\n",
      "0.6808139534883721\n",
      "0.6808139534883721\n",
      "0.6558139534883721\n",
      "0.6790697674418606\n",
      "0.7040697674418605\n",
      "0.6191860465116279\n",
      "0.6825581395348838\n",
      "0.6674418604651163\n",
      "0.6558139534883721\n",
      "0.6709302325581395\n",
      "0.6459302325581394\n",
      "0.5994186046511628\n",
      "0.7191860465116279\n",
      "0.6424418604651163\n",
      "0.7040697674418605\n",
      "0.6924418604651162\n",
      "0.6674418604651163\n",
      "0.6924418604651162\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6540697674418605\n",
      "Epoch 801/5000, Loss: 0.0039\n",
      "0.6540697674418605\n",
      "0.6906976744186047\n",
      "0.6540697674418605\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6540697674418605\n",
      "0.6308139534883721\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6290697674418604\n",
      "0.6406976744186047\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6808139534883721\n",
      "0.6959302325581395\n",
      "0.6691860465116279\n",
      "0.6558139534883721\n",
      "0.5959302325581395\n",
      "0.6441860465116279\n",
      "0.6924418604651162\n",
      "0.6325581395348837\n",
      "0.6575581395348836\n",
      "0.6325581395348837\n",
      "0.6058139534883721\n",
      "0.6790697674418606\n",
      "0.6575581395348836\n",
      "0.7075581395348837\n",
      "0.6558139534883721\n",
      "0.6924418604651162\n",
      "0.6825581395348838\n",
      "0.6575581395348836\n",
      "0.7308139534883721\n",
      "0.7058139534883721\n",
      "0.7058139534883721\n",
      "0.6825581395348838\n",
      "0.6959302325581395\n",
      "0.6941860465116279\n",
      "0.6843023255813954\n",
      "0.6441860465116279\n",
      "0.6494186046511627\n",
      "0.7406976744186048\n",
      "0.636046511627907\n",
      "0.6075581395348837\n",
      "0.6656976744186047\n",
      "0.5843023255813954\n",
      "0.6308139534883721\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.6424418604651163\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6325581395348837\n",
      "0.6691860465116279\n",
      "0.6441860465116279\n",
      "0.6226744186046512\n",
      "0.6459302325581394\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.6709302325581395\n",
      "0.6593023255813953\n",
      "0.6709302325581395\n",
      "0.6575581395348836\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6343023255813953\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6691860465116279\n",
      "0.6808139534883721\n",
      "0.6808139534883721\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6343023255813953\n",
      "0.6808139534883721\n",
      "0.6691860465116279\n",
      "0.6575581395348836\n",
      "0.6790697674418606\n",
      "0.6540697674418605\n",
      "0.6558139534883721\n",
      "0.6226744186046512\n",
      "0.6343023255813953\n",
      "0.6558139534883721\n",
      "0.6424418604651163\n",
      "0.5941860465116279\n",
      "0.6674418604651163\n",
      "0.6290697674418604\n",
      "0.6924418604651162\n",
      "0.6924418604651162\n",
      "0.6924418604651162\n",
      "0.6308139534883721\n",
      "0.6558139534883721\n",
      "0.6441860465116279\n",
      "0.6308139534883721\n",
      "0.6691860465116279\n",
      "0.7174418604651164\n",
      "0.7040697674418605\n",
      "0.6924418604651162\n",
      "0.6691860465116279\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6691860465116279\n",
      "0.6191860465116279\n",
      "0.6593023255813953\n",
      "0.5825581395348837\n",
      "0.6075581395348837\n",
      "0.6441860465116279\n",
      "0.5941860465116279\n",
      "0.6191860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6691860465116279\n",
      "0.5709302325581396\n",
      "0.6325581395348837\n",
      "0.6093023255813954\n",
      "0.6906976744186047\n",
      "0.7005813953488372\n",
      "0.5674418604651162\n",
      "0.7174418604651164\n",
      "0.7174418604651164\n",
      "0.6593023255813953\n",
      "0.6075581395348837\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6325581395348837\n",
      "0.6691860465116279\n",
      "0.6656976744186047\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6075581395348837\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6441860465116279\n",
      "0.7290697674418606\n",
      "0.7290697674418606\n",
      "0.7290697674418606\n",
      "0.6825581395348838\n",
      "0.6808139534883721\n",
      "0.7174418604651164\n",
      "0.7174418604651164\n",
      "0.6808139534883721\n",
      "0.6691860465116279\n",
      "0.7058139534883721\n",
      "0.6575581395348836\n",
      "0.6825581395348838\n",
      "0.6441860465116279\n",
      "0.6656976744186047\n",
      "0.6441860465116279\n",
      "0.6308139534883721\n",
      "0.6075581395348837\n",
      "0.6209302325581395\n",
      "0.6808139534883721\n",
      "0.6325581395348837\n",
      "0.6941860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6575581395348836\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.7075581395348837\n",
      "0.7058139534883721\n",
      "0.6941860465116279\n",
      "0.6325581395348837\n",
      "0.6808139534883721\n",
      "0.6558139534883721\n",
      "0.6691860465116279\n",
      "0.5959302325581395\n",
      "0.6441860465116279\n",
      "0.6459302325581394\n",
      "0.6593023255813953\n",
      "0.6825581395348838\n",
      "0.6691860465116279\n",
      "0.6308139534883721\n",
      "0.6441860465116279\n",
      "0.6790697674418606\n",
      "0.6459302325581394\n",
      "0.6308139534883721\n",
      "0.6459302325581394\n",
      "0.5593023255813954\n",
      "0.636046511627907\n",
      "0.6959302325581395\n",
      "0.6325581395348837\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6691860465116279\n",
      "Epoch 1001/5000, Loss: 0.0221\n",
      "0.6924418604651162\n",
      "0.6575581395348836\n",
      "0.6226744186046512\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6226744186046512\n",
      "0.611046511627907\n",
      "0.611046511627907\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6808139534883721\n",
      "0.6343023255813953\n",
      "0.6093023255813954\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6343023255813953\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6343023255813953\n",
      "0.6093023255813954\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.6476744186046511\n",
      "0.6476744186046511\n",
      "0.6476744186046511\n",
      "0.6209302325581395\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6808139534883721\n",
      "0.6924418604651162\n",
      "0.6575581395348836\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.636046511627907\n",
      "0.6226744186046512\n",
      "0.611046511627907\n",
      "0.586046511627907\n",
      "0.586046511627907\n",
      "0.5877906976744186\n",
      "0.6808139534883721\n",
      "0.6226744186046512\n",
      "0.6325581395348837\n",
      "0.6924418604651162\n",
      "0.6709302325581395\n",
      "0.6976744186046512\n",
      "0.6424418604651163\n",
      "0.6575581395348836\n",
      "0.6656976744186047\n",
      "0.6575581395348836\n",
      "0.7156976744186047\n",
      "0.6441860465116279\n",
      "0.6209302325581395\n",
      "0.6441860465116279\n",
      "0.6691860465116279\n",
      "0.6575581395348836\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.5726744186046512\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6593023255813953\n",
      "0.6093023255813954\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6709302325581395\n",
      "0.6325581395348837\n",
      "0.6593023255813953\n",
      "0.6593023255813953\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6209302325581395\n",
      "0.6691860465116279\n",
      "0.6808139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6441860465116279\n",
      "0.6691860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6325581395348837\n",
      "0.6691860465116279\n",
      "0.6459302325581394\n",
      "0.6825581395348838\n",
      "0.6941860465116279\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6540697674418605\n",
      "0.5459302325581394\n",
      "0.6424418604651163\n",
      "0.6709302325581395\n",
      "0.6441860465116279\n",
      "0.5976744186046512\n",
      "0.5959302325581395\n",
      "0.6093023255813954\n",
      "0.6226744186046512\n",
      "0.5959302325581395\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5726744186046512\n",
      "0.5709302325581396\n",
      "0.5593023255813954\n",
      "0.5709302325581396\n",
      "0.5476744186046512\n",
      "0.5593023255813954\n",
      "0.5476744186046512\n",
      "0.5476744186046512\n",
      "0.5476744186046512\n",
      "0.5593023255813954\n",
      "0.5843023255813954\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.6191860465116279\n",
      "0.5843023255813954\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.5726744186046512\n",
      "0.561046511627907\n",
      "0.561046511627907\n",
      "0.5726744186046512\n",
      "0.5726744186046512\n",
      "0.5726744186046512\n",
      "0.5726744186046512\n",
      "0.5976744186046512\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.5843023255813954\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.5843023255813954\n",
      "0.5726744186046512\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5726744186046512\n",
      "0.5843023255813954\n",
      "0.5959302325581395\n",
      "0.6209302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "Epoch 1201/5000, Loss: 0.0004\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5593023255813954\n",
      "0.5843023255813954\n",
      "0.5593023255813954\n",
      "0.5593023255813954\n",
      "0.5593023255813954\n",
      "0.5593023255813954\n",
      "0.5843023255813954\n",
      "0.5593023255813954\n",
      "0.5709302325581396\n",
      "0.5709302325581396\n",
      "0.5709302325581396\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.5709302325581396\n",
      "0.5959302325581395\n",
      "0.6540697674418605\n",
      "0.6627906976744187\n",
      "0.5691860465116279\n",
      "0.6459302325581394\n",
      "0.5691860465116279\n",
      "0.5924418604651163\n",
      "0.5656976744186047\n",
      "0.5505813953488372\n",
      "0.6058139534883721\n",
      "0.6040697674418606\n",
      "0.5691860465116279\n",
      "0.6424418604651163\n",
      "0.6058139534883721\n",
      "0.5825581395348837\n",
      "0.5808139534883722\n",
      "0.6058139534883721\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6424418604651163\n",
      "0.6174418604651163\n",
      "0.6540697674418605\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.6558139534883721\n",
      "0.6441860465116279\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6441860465116279\n",
      "0.6540697674418605\n",
      "0.6656976744186047\n",
      "0.5691860465116279\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6308139534883721\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6441860465116279\n",
      "0.6906976744186047\n",
      "0.5674418604651162\n",
      "0.6290697674418604\n",
      "0.6656976744186047\n",
      "0.6773255813953488\n",
      "0.6790697674418606\n",
      "0.6424418604651163\n",
      "0.702325581395349\n",
      "0.6790697674418606\n",
      "0.6558139534883721\n",
      "0.6790697674418606\n",
      "0.6674418604651163\n",
      "0.6174418604651163\n",
      "0.5808139534883722\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6424418604651163\n",
      "0.6406976744186047\n",
      "0.5808139534883722\n",
      "0.6523255813953488\n",
      "0.702325581395349\n",
      "0.6406976744186047\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6308139534883721\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.663953488372093\n",
      "0.6290697674418604\n",
      "0.6406976744186047\n",
      "0.6273255813953489\n",
      "0.6540697674418605\n",
      "0.6523255813953488\n",
      "0.5924418604651163\n",
      "0.5808139534883722\n",
      "0.5924418604651163\n",
      "0.6308139534883721\n",
      "0.6441860465116279\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.5825581395348837\n",
      "0.5941860465116279\n",
      "0.6540697674418605\n",
      "0.5674418604651162\n",
      "0.6674418604651163\n",
      "0.6424418604651163\n",
      "0.5575581395348838\n",
      "0.5959302325581395\n",
      "0.5906976744186047\n",
      "0.502906976744186\n",
      "0.5906976744186047\n",
      "0.5575581395348838\n",
      "0.663953488372093\n",
      "0.5674418604651162\n",
      "0.6191860465116279\n",
      "0.6023255813953488\n",
      "0.6691860465116279\n",
      "0.7058139534883721\n",
      "0.7058139534883721\n",
      "0.7174418604651164\n",
      "0.7040697674418605\n",
      "0.6558139534883721\n",
      "0.6808139534883721\n",
      "0.6924418604651162\n",
      "0.6540697674418605\n",
      "0.6674418604651163\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.702325581395349\n",
      "0.6656976744186047\n",
      "0.6389534883720931\n",
      "0.6459302325581394\n",
      "0.6441860465116279\n",
      "0.7273255813953488\n",
      "0.6790697674418606\n",
      "0.6209302325581395\n",
      "0.6924418604651162\n",
      "0.7156976744186047\n",
      "0.7040697674418605\n",
      "0.7156976744186047\n",
      "0.7389534883720931\n",
      "0.7273255813953488\n",
      "0.6924418604651162\n",
      "0.6924418604651162\n",
      "0.6808139534883721\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.702325581395349\n",
      "0.688953488372093\n",
      "0.6441860465116279\n",
      "0.6906976744186047\n",
      "0.6790697674418606\n",
      "0.5808139534883722\n",
      "0.5825581395348837\n",
      "0.6174418604651163\n",
      "0.6040697674418606\n",
      "0.702325581395349\n",
      "0.6058139534883721\n",
      "0.5441860465116279\n",
      "0.6906976744186047\n",
      "0.6843023255813954\n",
      "0.5941860465116279\n",
      "0.6540697674418605\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6075581395348837\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6441860465116279\n",
      "0.5825581395348837\n",
      "0.5709302325581396\n",
      "0.6325581395348837\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "Epoch 1401/5000, Loss: 0.0063\n",
      "0.6191860465116279\n",
      "0.6790697674418606\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6209302325581395\n",
      "0.6075581395348837\n",
      "0.6325581395348837\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6924418604651162\n",
      "0.6924418604651162\n",
      "0.6924418604651162\n",
      "0.6790697674418606\n",
      "0.7040697674418605\n",
      "0.6924418604651162\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6691860465116279\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6558139534883721\n",
      "0.6825581395348838\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6674418604651163\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6441860465116279\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6290697674418604\n",
      "0.6674418604651163\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6674418604651163\n",
      "0.6209302325581395\n",
      "0.6441860465116279\n",
      "0.6424418604651163\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6424418604651163\n",
      "0.6174418604651163\n",
      "0.5924418604651163\n",
      "0.6191860465116279\n",
      "0.6156976744186047\n",
      "0.5459302325581394\n",
      "0.6209302325581395\n",
      "0.6424418604651163\n",
      "0.6441860465116279\n",
      "0.6075581395348837\n",
      "0.6191860465116279\n",
      "0.6691860465116279\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6691860465116279\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6674418604651163\n",
      "0.6424418604651163\n",
      "0.6656976744186047\n",
      "0.6424418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.6773255813953488\n",
      "0.6790697674418606\n",
      "0.6308139534883721\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5941860465116279\n",
      "0.6058139534883721\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6058139534883721\n",
      "0.6040697674418606\n",
      "0.6209302325581395\n",
      "0.5709302325581396\n",
      "0.6476744186046511\n",
      "0.6790697674418606\n",
      "0.5959302325581395\n",
      "0.6325581395348837\n",
      "0.5808139534883722\n",
      "0.6406976744186047\n",
      "0.5825581395348837\n",
      "0.6523255813953488\n",
      "0.5924418604651163\n",
      "0.6674418604651163\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.5941860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.6191860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5825581395348837\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5691860465116279\n",
      "0.5575581395348838\n",
      "0.5825581395348837\n",
      "0.5941860465116279\n",
      "0.5808139534883722\n",
      "0.6174418604651163\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.5941860465116279\n",
      "0.6058139534883721\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.5941860465116279\n",
      "0.6058139534883721\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6441860465116279\n",
      "0.6674418604651163\n",
      "0.6924418604651162\n",
      "0.6924418604651162\n",
      "0.6674418604651163\n",
      "0.7058139534883721\n",
      "0.6075581395348837\n",
      "0.5825581395348837\n",
      "0.6058139534883721\n",
      "0.5941860465116279\n",
      "0.5709302325581396\n",
      "0.5825581395348837\n",
      "0.6773255813953488\n",
      "0.6325581395348837\n",
      "0.702325581395349\n",
      "0.6906976744186047\n",
      "0.6308139534883721\n",
      "0.6575581395348836\n",
      "Epoch 1601/5000, Loss: 0.0074\n",
      "0.6441860465116279\n",
      "0.6075581395348837\n",
      "0.6540697674418605\n",
      "0.6691860465116279\n",
      "0.5709302325581396\n",
      "0.611046511627907\n",
      "0.6459302325581394\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6174418604651163\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6441860465116279\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6575581395348836\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6325581395348837\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6325581395348837\n",
      "0.6808139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6441860465116279\n",
      "0.6808139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6691860465116279\n",
      "0.6441860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6808139534883721\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6441860465116279\n",
      "0.6075581395348837\n",
      "0.6924418604651162\n",
      "0.6674418604651163\n",
      "0.6691860465116279\n",
      "0.6191860465116279\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.6191860465116279\n",
      "0.6325581395348837\n",
      "0.5825581395348837\n",
      "0.5941860465116279\n",
      "0.611046511627907\n",
      "0.5976744186046512\n",
      "0.5843023255813954\n",
      "0.6058139534883721\n",
      "0.6441860465116279\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6075581395348837\n",
      "0.6058139534883721\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.6656976744186047\n",
      "0.6674418604651163\n",
      "0.6424418604651163\n",
      "0.6941860465116279\n",
      "0.6343023255813953\n",
      "0.6941860465116279\n",
      "0.6808139534883721\n",
      "0.6441860465116279\n",
      "0.6308139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6790697674418606\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6790697674418606\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6558139534883721\n",
      "0.6808139534883721\n",
      "0.6808139534883721\n",
      "0.6808139534883721\n",
      "0.6808139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6808139534883721\n",
      "0.6558139534883721\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6441860465116279\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6924418604651162\n",
      "0.6924418604651162\n",
      "0.7040697674418605\n",
      "0.7156976744186047\n",
      "0.7156976744186047\n",
      "0.6808139534883721\n",
      "0.6325581395348837\n",
      "0.6906976744186047\n",
      "0.6290697674418604\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6174418604651163\n",
      "0.6656976744186047\n",
      "0.555813953488372\n",
      "0.6325581395348837\n",
      "0.6459302325581394\n",
      "0.6075581395348837\n",
      "0.6459302325581394\n",
      "0.6406976744186047\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6308139534883721\n",
      "0.6040697674418606\n",
      "0.5476744186046512\n",
      "0.6906976744186047\n",
      "0.6808139534883721\n",
      "0.6674418604651163\n",
      "0.6191860465116279\n",
      "0.6174418604651163\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.6174418604651163\n",
      "0.6674418604651163\n",
      "0.6558139534883721\n",
      "0.5924418604651163\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.6058139534883721\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5941860465116279\n",
      "0.6191860465116279\n",
      "Epoch 1801/5000, Loss: 0.0016\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6441860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.6441860465116279\n",
      "0.6558139534883721\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6040697674418606\n",
      "0.5906976744186047\n",
      "0.6040697674418606\n",
      "0.6406976744186047\n",
      "0.6540697674418605\n",
      "0.6290697674418604\n",
      "0.6523255813953488\n",
      "0.7058139534883721\n",
      "0.6808139534883721\n",
      "0.6674418604651163\n",
      "0.6040697674418606\n",
      "0.6290697674418604\n",
      "0.6058139534883721\n",
      "0.6023255813953488\n",
      "0.6023255813953488\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5906976744186047\n",
      "0.5906976744186047\n",
      "0.5906976744186047\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.6174418604651163\n",
      "0.713953488372093\n",
      "0.6656976744186047\n",
      "0.6790697674418606\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6441860465116279\n",
      "0.6174418604651163\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.5941860465116279\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.5924418604651163\n",
      "0.6040697674418606\n",
      "0.6424418604651163\n",
      "0.6924418604651162\n",
      "0.6808139534883721\n",
      "0.6174418604651163\n",
      "0.6273255813953489\n",
      "0.6441860465116279\n",
      "0.588953488372093\n",
      "0.6558139534883721\n",
      "0.5691860465116279\n",
      "0.5924418604651163\n",
      "0.6540697674418605\n",
      "0.5325581395348837\n",
      "0.5209302325581395\n",
      "0.6075581395348837\n",
      "0.5575581395348838\n",
      "0.5441860465116279\n",
      "0.5191860465116279\n",
      "0.5441860465116279\n",
      "0.5441860465116279\n",
      "0.5325581395348837\n",
      "0.5209302325581395\n",
      "0.5575581395348838\n",
      "0.5209302325581395\n",
      "0.5209302325581395\n",
      "0.5209302325581395\n",
      "0.5209302325581395\n",
      "0.5325581395348837\n",
      "0.5441860465116279\n",
      "0.5441860465116279\n",
      "0.5441860465116279\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.5325581395348837\n",
      "0.6058139534883721\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.5808139534883722\n",
      "0.6058139534883721\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.6040697674418606\n",
      "0.5924418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5325581395348837\n",
      "0.5325581395348837\n",
      "0.5575581395348838\n",
      "0.5441860465116279\n",
      "0.5441860465116279\n",
      "0.5441860465116279\n",
      "0.5441860465116279\n",
      "0.555813953488372\n",
      "0.5441860465116279\n",
      "0.5691860465116279\n",
      "0.5441860465116279\n",
      "0.5441860465116279\n",
      "0.5808139534883722\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5691860465116279\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5691860465116279\n",
      "0.5575581395348838\n",
      "0.6174418604651163\n",
      "0.563953488372093\n",
      "0.5406976744186047\n",
      "0.5343023255813953\n",
      "0.5790697674418605\n",
      "0.5424418604651163\n",
      "0.5308139534883721\n",
      "0.5308139534883721\n",
      "0.5308139534883721\n",
      "0.5308139534883721\n",
      "0.5308139534883721\n",
      "0.5191860465116279\n",
      "0.5424418604651163\n",
      "Epoch 2001/5000, Loss: 0.0007\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5424418604651163\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.555813953488372\n",
      "0.5674418604651162\n",
      "0.5424418604651163\n",
      "0.5674418604651162\n",
      "0.5290697674418605\n",
      "0.7040697674418605\n",
      "0.588953488372093\n",
      "0.613953488372093\n",
      "0.4726744186046512\n",
      "0.5674418604651162\n",
      "0.6523255813953488\n",
      "0.5790697674418605\n",
      "0.6156976744186047\n",
      "0.6058139534883721\n",
      "0.5441860465116279\n",
      "0.5924418604651163\n",
      "0.6058139534883721\n",
      "0.5709302325581396\n",
      "0.6325581395348837\n",
      "0.6075581395348837\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6325581395348837\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6441860465116279\n",
      "0.6209302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5843023255813954\n",
      "0.5593023255813954\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.6191860465116279\n",
      "0.7156976744186047\n",
      "0.6755813953488372\n",
      "0.563953488372093\n",
      "0.5773255813953488\n",
      "0.5872093023255813\n",
      "0.6156976744186047\n",
      "0.5941860465116279\n",
      "0.6191860465116279\n",
      "0.5924418604651163\n",
      "0.6058139534883721\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.6040697674418606\n",
      "0.5906976744186047\n",
      "0.5790697674418605\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.5808139534883722\n",
      "0.5790697674418605\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6174418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6023255813953488\n",
      "0.5906976744186047\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5674418604651162\n",
      "0.5441860465116279\n",
      "0.5906976744186047\n",
      "0.5674418604651162\n",
      "0.6040697674418606\n",
      "0.6156976744186047\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.5906976744186047\n",
      "0.5924418604651163\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.6174418604651163\n",
      "0.5843023255813954\n",
      "0.5459302325581394\n",
      "0.5093023255813953\n",
      "0.5441860465116279\n",
      "0.5924418604651163\n",
      "0.6540697674418605\n",
      "0.5075581395348837\n",
      "0.6308139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6424418604651163\n",
      "0.6540697674418605\n",
      "0.6290697674418604\n",
      "0.6058139534883721\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6575581395348836\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6558139534883721\n",
      "0.6441860465116279\n",
      "0.6075581395348837\n",
      "0.6040697674418606\n",
      "0.6308139534883721\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6691860465116279\n",
      "0.6808139534883721\n",
      "0.6924418604651162\n",
      "0.5976744186046512\n",
      "0.6174418604651163\n",
      "0.663953488372093\n",
      "0.5593023255813954\n",
      "0.5924418604651163\n",
      "0.6308139534883721\n",
      "0.5494186046511628\n",
      "0.6023255813953488\n",
      "0.6343023255813953\n",
      "0.5325581395348837\n",
      "0.5575581395348838\n",
      "0.6075581395348837\n",
      "0.6325581395348837\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "Epoch 2201/5000, Loss: 0.0004\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6308139534883721\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6174418604651163\n",
      "0.6058139534883721\n",
      "0.6290697674418604\n",
      "0.5941860465116279\n",
      "0.5244186046511627\n",
      "0.6459302325581394\n",
      "0.6040697674418606\n",
      "0.5773255813953488\n",
      "0.5191860465116279\n",
      "0.5575581395348838\n",
      "0.6191860465116279\n",
      "0.5691860465116279\n",
      "0.6174418604651163\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6290697674418604\n",
      "0.5924418604651163\n",
      "0.5691860465116279\n",
      "0.5325581395348837\n",
      "0.5825581395348837\n",
      "0.5709302325581396\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.6209302325581395\n",
      "0.6441860465116279\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6093023255813954\n",
      "0.6459302325581394\n",
      "0.5941860465116279\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5459302325581394\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5941860465116279\n",
      "0.6058139534883721\n",
      "0.5825581395348837\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.6040697674418606\n",
      "0.6174418604651163\n",
      "0.6424418604651163\n",
      "0.6656976744186047\n",
      "0.6424418604651163\n",
      "0.5941860465116279\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.5825581395348837\n",
      "0.5941860465116279\n",
      "0.5691860465116279\n",
      "0.5575581395348838\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6058139534883721\n",
      "0.5924418604651163\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.5808139534883722\n",
      "0.6558139534883721\n",
      "0.6191860465116279\n",
      "0.6058139534883721\n",
      "0.563953488372093\n",
      "0.5523255813953488\n",
      "0.5540697674418605\n",
      "0.5906976744186047\n",
      "0.5906976744186047\n",
      "0.6040697674418606\n",
      "0.555813953488372\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5924418604651163\n",
      "0.5825581395348837\n",
      "0.5790697674418605\n",
      "0.5325581395348837\n",
      "0.5691860465116279\n",
      "0.5941860465116279\n",
      "0.5575581395348838\n",
      "0.5441860465116279\n",
      "0.5790697674418605\n",
      "0.6273255813953489\n",
      "0.613953488372093\n",
      "0.6389534883720931\n",
      "0.6174418604651163\n",
      "0.6424418604651163\n",
      "0.6290697674418604\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.5656976744186047\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.6040697674418606\n",
      "0.5924418604651163\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5924418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.5808139534883722\n",
      "Epoch 2401/5000, Loss: 0.0002\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.6656976744186047\n",
      "0.6023255813953488\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6406976744186047\n",
      "0.6058139534883721\n",
      "0.6540697674418605\n",
      "0.6906976744186047\n",
      "0.5906976744186047\n",
      "0.6058139534883721\n",
      "0.6191860465116279\n",
      "0.6406976744186047\n",
      "0.6558139534883721\n",
      "0.6691860465116279\n",
      "0.536046511627907\n",
      "0.5593023255813954\n",
      "0.6441860465116279\n",
      "0.6691860465116279\n",
      "0.6808139534883721\n",
      "0.6424418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.7174418604651164\n",
      "0.6441860465116279\n",
      "0.6941860465116279\n",
      "0.6825581395348838\n",
      "0.7058139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6790697674418606\n",
      "0.6674418604651163\n",
      "0.6924418604651162\n",
      "0.6808139534883721\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6343023255813953\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.7174418604651164\n",
      "0.7290697674418606\n",
      "0.702325581395349\n",
      "0.613953488372093\n",
      "0.6174418604651163\n",
      "0.6540697674418605\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6540697674418605\n",
      "0.7156976744186047\n",
      "0.6924418604651162\n",
      "0.6424418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.6174418604651163\n",
      "0.6906976744186047\n",
      "0.6790697674418606\n",
      "0.6906976744186047\n",
      "0.6406976744186047\n",
      "0.6023255813953488\n",
      "0.6389534883720931\n",
      "0.702325581395349\n",
      "0.555813953488372\n",
      "0.5691860465116279\n",
      "0.588953488372093\n",
      "0.6511627906976745\n",
      "0.663953488372093\n",
      "0.6941860465116279\n",
      "0.7075581395348837\n",
      "0.7191860465116279\n",
      "0.7191860465116279\n",
      "0.7075581395348837\n",
      "0.7191860465116279\n",
      "0.7075581395348837\n",
      "0.6959302325581395\n",
      "0.6709302325581395\n",
      "0.6691860465116279\n",
      "0.6924418604651162\n",
      "0.6924418604651162\n",
      "0.6924418604651162\n",
      "0.6924418604651162\n",
      "0.6674418604651163\n",
      "0.6808139534883721\n",
      "0.6808139534883721\n",
      "0.6808139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6924418604651162\n",
      "0.6924418604651162\n",
      "0.7174418604651164\n",
      "0.7174418604651164\n",
      "0.7058139534883721\n",
      "0.6941860465116279\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6941860465116279\n",
      "0.6691860465116279\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6808139534883721\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6825581395348838\n",
      "0.6593023255813953\n",
      "0.6593023255813953\n",
      "0.6593023255813953\n",
      "0.6825581395348838\n",
      "0.6709302325581395\n",
      "0.6709302325581395\n",
      "0.6941860465116279\n",
      "0.6941860465116279\n",
      "0.6575581395348836\n",
      "0.6691860465116279\n",
      "0.6825581395348838\n",
      "0.7058139534883721\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6441860465116279\n",
      "0.6674418604651163\n",
      "0.6558139534883721\n",
      "0.6808139534883721\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6790697674418606\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "Epoch 2601/5000, Loss: 0.0014\n",
      "0.6906976744186047\n",
      "0.6674418604651163\n",
      "0.6790697674418606\n",
      "0.6424418604651163\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6674418604651163\n",
      "0.6808139534883721\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6656976744186047\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6656976744186047\n",
      "0.6773255813953488\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6906976744186047\n",
      "0.7156976744186047\n",
      "0.6906976744186047\n",
      "0.702325581395349\n",
      "0.6790697674418606\n",
      "0.5941860465116279\n",
      "0.6406976744186047\n",
      "0.5924418604651163\n",
      "0.6558139534883721\n",
      "0.561046511627907\n",
      "0.5424418604651163\n",
      "0.5843023255813954\n",
      "0.5726744186046512\n",
      "0.5843023255813954\n",
      "0.5976744186046512\n",
      "0.5959302325581395\n",
      "0.5825581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.5941860465116279\n",
      "0.6058139534883721\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.5825581395348837\n",
      "0.5691860465116279\n",
      "0.5941860465116279\n",
      "0.6174418604651163\n",
      "0.6656976744186047\n",
      "0.6005813953488373\n",
      "0.5924418604651163\n",
      "0.6058139534883721\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6023255813953488\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6424418604651163\n",
      "0.6075581395348837\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6389534883720931\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6226744186046512\n",
      "0.6226744186046512\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6459302325581394\n",
      "0.6191860465116279\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6058139534883721\n",
      "0.6191860465116279\n",
      "0.5808139534883722\n",
      "0.6023255813953488\n",
      "0.5773255813953488\n",
      "0.5906976744186047\n",
      "0.5790697674418605\n",
      "0.5709302325581396\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.5459302325581394\n",
      "0.5674418604651162\n",
      "0.5924418604651163\n",
      "0.6441860465116279\n",
      "0.5924418604651163\n",
      "0.6023255813953488\n",
      "0.588953488372093\n",
      "0.6505813953488373\n",
      "0.6389534883720931\n",
      "0.6308139534883721\n",
      "0.6808139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6790697674418606\n",
      "0.7040697674418605\n",
      "0.7040697674418605\n",
      "0.6790697674418606\n",
      "0.6906976744186047\n",
      "0.7156976744186047\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6656976744186047\n",
      "0.6540697674418605\n",
      "0.6424418604651163\n",
      "0.6808139534883721\n",
      "0.6308139534883721\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6906976744186047\n",
      "0.6808139534883721\n",
      "0.6808139534883721\n",
      "0.6924418604651162\n",
      "0.6924418604651162\n",
      "0.7174418604651164\n",
      "0.7406976744186048\n",
      "0.7406976744186048\n",
      "0.7406976744186048\n",
      "Epoch 2801/5000, Loss: 0.0059\n",
      "0.6558139534883721\n",
      "0.6191860465116279\n",
      "0.6808139534883721\n",
      "0.6808139534883721\n",
      "0.5825581395348837\n",
      "0.6191860465116279\n",
      "0.6424418604651163\n",
      "0.702325581395349\n",
      "0.6558139534883721\n",
      "0.5924418604651163\n",
      "0.555813953488372\n",
      "0.5825581395348837\n",
      "0.5575581395348838\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.6040697674418606\n",
      "0.6191860465116279\n",
      "0.6674418604651163\n",
      "0.6540697674418605\n",
      "0.6308139534883721\n",
      "0.6075581395348837\n",
      "0.6325581395348837\n",
      "0.6558139534883721\n",
      "0.6790697674418606\n",
      "0.6906976744186047\n",
      "0.6656976744186047\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6790697674418606\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6174418604651163\n",
      "0.5924418604651163\n",
      "0.5825581395348837\n",
      "0.6558139534883721\n",
      "0.6441860465116279\n",
      "0.6459302325581394\n",
      "0.6441860465116279\n",
      "0.6023255813953488\n",
      "0.6023255813953488\n",
      "0.5924418604651163\n",
      "0.6040697674418606\n",
      "0.5656976744186047\n",
      "0.5906976744186047\n",
      "0.6540697674418605\n",
      "0.6156976744186047\n",
      "0.6040697674418606\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6406976744186047\n",
      "0.6523255813953488\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6273255813953489\n",
      "0.6773255813953488\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.5290697674418605\n",
      "0.4906976744186047\n",
      "0.5523255813953488\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.6406976744186047\n",
      "0.6906976744186047\n",
      "0.6406976744186047\n",
      "0.6174418604651163\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6290697674418604\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6656976744186047\n",
      "0.6540697674418605\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6290697674418604\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6540697674418605\n",
      "0.6656976744186047\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.5691860465116279\n",
      "0.6290697674418604\n",
      "0.6058139534883721\n",
      "0.5808139534883722\n",
      "0.6325581395348837\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.6226744186046512\n",
      "0.6343023255813953\n",
      "0.6226744186046512\n",
      "0.6226744186046512\n",
      "0.6226744186046512\n",
      "0.6226744186046512\n",
      "0.6424418604651163\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.6058139534883721\n",
      "0.5808139534883722\n",
      "0.5191860465116279\n",
      "0.6075581395348837\n",
      "0.5726744186046512\n",
      "0.5761627906976744\n",
      "0.6261627906976744\n",
      "0.46279069767441866\n",
      "0.3843023255813953\n",
      "0.6476744186046511\n",
      "0.5424418604651163\n",
      "0.561046511627907\n",
      "0.5424418604651163\n",
      "0.5476744186046512\n",
      "0.5459302325581394\n",
      "0.5825581395348837\n",
      "0.5726744186046512\n",
      "0.6093023255813954\n",
      "0.5459302325581394\n",
      "0.5593023255813954\n",
      "0.5709302325581396\n",
      "0.5476744186046512\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6075581395348837\n",
      "0.6191860465116279\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.6093023255813954\n",
      "0.6209302325581395\n",
      "0.6441860465116279\n",
      "0.5959302325581395\n",
      "0.5726744186046512\n",
      "0.611046511627907\n",
      "0.611046511627907\n",
      "0.611046511627907\n",
      "0.5976744186046512\n",
      "0.6459302325581394\n",
      "0.6575581395348836\n",
      "0.6459302325581394\n",
      "0.6343023255813953\n",
      "0.5843023255813954\n",
      "0.5709302325581396\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "Epoch 3001/5000, Loss: 0.0050\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6093023255813954\n",
      "0.5976744186046512\n",
      "0.5976744186046512\n",
      "0.5976744186046512\n",
      "0.5976744186046512\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.6209302325581395\n",
      "0.5959302325581395\n",
      "0.5941860465116279\n",
      "0.5494186046511628\n",
      "0.5377906976744187\n",
      "0.561046511627907\n",
      "0.5377906976744187\n",
      "0.5377906976744187\n",
      "0.5494186046511628\n",
      "0.5976744186046512\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6325581395348837\n",
      "0.5976744186046512\n",
      "0.5627906976744187\n",
      "0.5627906976744187\n",
      "0.5744186046511628\n",
      "0.586046511627907\n",
      "0.586046511627907\n",
      "0.586046511627907\n",
      "0.5976744186046512\n",
      "0.5976744186046512\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6441860465116279\n",
      "0.6075581395348837\n",
      "0.6308139534883721\n",
      "0.6093023255813954\n",
      "0.5843023255813954\n",
      "0.6093023255813954\n",
      "0.6441860465116279\n",
      "0.5377906976744187\n",
      "0.5825581395348837\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.6325581395348837\n",
      "0.5593023255813954\n",
      "0.6075581395348837\n",
      "0.5726744186046512\n",
      "0.6226744186046512\n",
      "0.4994186046511628\n",
      "0.6244186046511628\n",
      "0.5976744186046512\n",
      "0.6343023255813953\n",
      "0.5843023255813954\n",
      "0.5494186046511628\n",
      "0.5843023255813954\n",
      "0.6209302325581395\n",
      "0.5593023255813954\n",
      "0.5593023255813954\n",
      "0.5709302325581396\n",
      "0.5843023255813954\n",
      "0.5726744186046512\n",
      "0.5593023255813954\n",
      "0.5709302325581396\n",
      "0.5843023255813954\n",
      "0.5476744186046512\n",
      "0.5593023255813954\n",
      "0.5593023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.5843023255813954\n",
      "0.6093023255813954\n",
      "0.6709302325581395\n",
      "0.5726744186046512\n",
      "0.6226744186046512\n",
      "0.5726744186046512\n",
      "0.5593023255813954\n",
      "0.5959302325581395\n",
      "0.5691860465116279\n",
      "0.5593023255813954\n",
      "0.5877906976744186\n",
      "0.6011627906976744\n",
      "0.5877906976744186\n",
      "0.5994186046511628\n",
      "0.5994186046511628\n",
      "0.5261627906976745\n",
      "0.5494186046511628\n",
      "0.5726744186046512\n",
      "0.5959302325581395\n",
      "0.5709302325581396\n",
      "0.5959302325581395\n",
      "0.5709302325581396\n",
      "0.5976744186046512\n",
      "0.511046511627907\n",
      "0.5843023255813954\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.6209302325581395\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6441860465116279\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.5691860465116279\n",
      "0.6191860465116279\n",
      "0.5343023255813953\n",
      "0.6093023255813954\n",
      "0.6593023255813953\n",
      "0.6691860465116279\n",
      "0.6924418604651162\n",
      "0.6790697674418606\n",
      "0.6406976744186047\n",
      "0.6325581395348837\n",
      "0.6924418604651162\n",
      "0.6790697674418606\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6290697674418604\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6308139534883721\n",
      "0.6290697674418604\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6906976744186047\n",
      "0.6023255813953488\n",
      "0.688953488372093\n",
      "0.702325581395349\n",
      "0.702325581395349\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6674418604651163\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.5941860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "Epoch 3201/5000, Loss: 0.0004\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.561046511627907\n",
      "0.5593023255813954\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5709302325581396\n",
      "0.6075581395348837\n",
      "0.5843023255813954\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.6191860465116279\n",
      "0.5825581395348837\n",
      "0.6808139534883721\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6325581395348837\n",
      "0.5627906976744187\n",
      "0.586046511627907\n",
      "0.586046511627907\n",
      "0.5691860465116279\n",
      "0.6075581395348837\n",
      "0.7040697674418605\n",
      "0.586046511627907\n",
      "0.6244186046511628\n",
      "0.636046511627907\n",
      "0.6476744186046511\n",
      "0.6343023255813953\n",
      "0.6226744186046512\n",
      "0.6226744186046512\n",
      "0.6226744186046512\n",
      "0.6226744186046512\n",
      "0.6343023255813953\n",
      "0.6343023255813953\n",
      "0.6226744186046512\n",
      "0.6343023255813953\n",
      "0.6226744186046512\n",
      "0.6226744186046512\n",
      "0.6226744186046512\n",
      "0.6226744186046512\n",
      "0.5976744186046512\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6343023255813953\n",
      "0.6244186046511628\n",
      "0.6244186046511628\n",
      "0.6244186046511628\n",
      "0.6244186046511628\n",
      "0.6244186046511628\n",
      "0.636046511627907\n",
      "0.636046511627907\n",
      "0.6593023255813953\n",
      "0.636046511627907\n",
      "0.611046511627907\n",
      "0.6343023255813953\n",
      "0.6093023255813954\n",
      "0.6308139534883721\n",
      "0.6325581395348837\n",
      "0.5674418604651162\n",
      "0.5476744186046512\n",
      "0.5575581395348838\n",
      "0.5691860465116279\n",
      "0.555813953488372\n",
      "0.45755813953488367\n",
      "0.461046511627907\n",
      "0.6075581395348837\n",
      "0.5709302325581396\n",
      "0.555813953488372\n",
      "0.6459302325581394\n",
      "0.6226744186046512\n",
      "0.536046511627907\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.5843023255813954\n",
      "0.5593023255813954\n",
      "0.6209302325581395\n",
      "0.5726744186046512\n",
      "0.5226744186046511\n",
      "0.5226744186046511\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5726744186046512\n",
      "0.5593023255813954\n",
      "0.6191860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.6058139534883721\n",
      "0.6441860465116279\n",
      "0.6906976744186047\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6656976744186047\n",
      "0.6540697674418605\n",
      "0.6656976744186047\n",
      "0.5790697674418605\n",
      "0.5674418604651162\n",
      "0.5790697674418605\n",
      "0.5924418604651163\n",
      "0.6523255813953488\n",
      "0.6773255813953488\n",
      "0.6773255813953488\n",
      "0.6790697674418606\n",
      "0.6674418604651163\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6674418604651163\n",
      "0.6790697674418606\n",
      "0.702325581395349\n",
      "0.6040697674418606\n",
      "0.6174418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6674418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6540697674418605\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6424418604651163\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "Epoch 3401/5000, Loss: 0.0001\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.5941860465116279\n",
      "0.6540697674418605\n",
      "0.6424418604651163\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.5441860465116279\n",
      "0.6156976744186047\n",
      "0.5773255813953488\n",
      "0.43255813953488376\n",
      "0.555813953488372\n",
      "0.6540697674418605\n",
      "0.6209302325581395\n",
      "0.6058139534883721\n",
      "0.5825581395348837\n",
      "0.6558139534883721\n",
      "0.6191860465116279\n",
      "0.5709302325581396\n",
      "0.5825581395348837\n",
      "0.5709302325581396\n",
      "0.5593023255813954\n",
      "0.5593023255813954\n",
      "0.5959302325581395\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6441860465116279\n",
      "0.6075581395348837\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6558139534883721\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6325581395348837\n",
      "0.6558139534883721\n",
      "0.6308139534883721\n",
      "0.5825581395348837\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6308139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6691860465116279\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6191860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6441860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6441860465116279\n",
      "0.6075581395348837\n",
      "0.6593023255813953\n",
      "0.6476744186046511\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6093023255813954\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6575581395348836\n",
      "0.6709302325581395\n",
      "0.611046511627907\n",
      "0.588953488372093\n",
      "0.6011627906976744\n",
      "0.6691860465116279\n",
      "0.6691860465116279\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.5843023255813954\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6441860465116279\n",
      "0.5976744186046512\n",
      "0.6209302325581395\n",
      "0.6075581395348837\n",
      "0.6424418604651163\n",
      "0.6674418604651163\n",
      "0.6174418604651163\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6558139534883721\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6674418604651163\n",
      "0.6790697674418606\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6290697674418604\n",
      "0.6406976744186047\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "Epoch 3601/5000, Loss: 0.0098\n",
      "0.5959302325581395\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6575581395348836\n",
      "0.6575581395348836\n",
      "0.6674418604651163\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6540697674418605\n",
      "0.6656976744186047\n",
      "0.6540697674418605\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6540697674418605\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6656976744186047\n",
      "0.702325581395349\n",
      "0.702325581395349\n",
      "0.702325581395349\n",
      "0.702325581395349\n",
      "0.702325581395349\n",
      "0.702325581395349\n",
      "0.702325581395349\n",
      "0.702325581395349\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6906976744186047\n",
      "0.6790697674418606\n",
      "0.6290697674418604\n",
      "0.6406976744186047\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6656976744186047\n",
      "0.6540697674418605\n",
      "0.6790697674418606\n",
      "0.6540697674418605\n",
      "0.6424418604651163\n",
      "0.5191860465116279\n",
      "0.6058139534883721\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.5290697674418605\n",
      "0.6040697674418606\n",
      "0.6023255813953488\n",
      "0.5656976744186047\n",
      "0.5790697674418605\n",
      "0.5441860465116279\n",
      "0.5691860465116279\n",
      "0.5441860465116279\n",
      "0.5906976744186047\n",
      "0.5790697674418605\n",
      "0.5674418604651162\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5540697674418605\n",
      "0.5424418604651163\n",
      "0.5424418604651163\n",
      "0.5424418604651163\n",
      "0.5308139534883721\n",
      "0.5424418604651163\n",
      "0.5308139534883721\n",
      "0.5691860465116279\n",
      "0.5808139534883722\n",
      "0.5941860465116279\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.6058139534883721\n",
      "0.5941860465116279\n",
      "0.5191860465116279\n",
      "0.5191860465116279\n",
      "0.5325581395348837\n",
      "0.5325581395348837\n",
      "0.5325581395348837\n",
      "0.5575581395348838\n",
      "0.5209302325581395\n",
      "0.5441860465116279\n",
      "0.5441860465116279\n",
      "0.5441860465116279\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.5674418604651162\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.6040697674418606\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.6040697674418606\n",
      "0.5924418604651163\n",
      "0.5674418604651162\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5674418604651162\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5424418604651163\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.555813953488372\n",
      "0.555813953488372\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.555813953488372\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5790697674418605\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5726744186046512\n",
      "0.5709302325581396\n",
      "0.5691860465116279\n",
      "0.6325581395348837\n",
      "0.6075581395348837\n",
      "0.6058139534883721\n",
      "0.555813953488372\n",
      "0.5575581395348838\n",
      "0.6441860465116279\n",
      "0.6174418604651163\n",
      "0.6540697674418605\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.5674418604651162\n",
      "0.555813953488372\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.5906976744186047\n",
      "0.586046511627907\n",
      "0.6308139534883721\n",
      "0.5691860465116279\n",
      "0.5808139534883722\n",
      "0.6174418604651163\n",
      "0.5825581395348837\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.5825581395348837\n",
      "0.5709302325581396\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5709302325581396\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5843023255813954\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6325581395348837\n",
      "Epoch 3801/5000, Loss: 0.0008\n",
      "0.6209302325581395\n",
      "0.6191860465116279\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.6191860465116279\n",
      "0.5691860465116279\n",
      "0.5325581395348837\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5691860465116279\n",
      "0.5941860465116279\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5709302325581396\n",
      "0.5959302325581395\n",
      "0.5709302325581396\n",
      "0.5709302325581396\n",
      "0.5709302325581396\n",
      "0.5709302325581396\n",
      "0.5825581395348837\n",
      "0.5709302325581396\n",
      "0.5709302325581396\n",
      "0.5709302325581396\n",
      "0.5709302325581396\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.5575581395348838\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5843023255813954\n",
      "0.6191860465116279\n",
      "0.5924418604651163\n",
      "0.48604651162790696\n",
      "0.5959302325581395\n",
      "0.5825581395348837\n",
      "0.6040697674418606\n",
      "0.6459302325581394\n",
      "0.6209302325581395\n",
      "0.5959302325581395\n",
      "0.5941860465116279\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6808139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6575581395348836\n",
      "0.6674418604651163\n",
      "0.6790697674418606\n",
      "0.6540697674418605\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6424418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.6540697674418605\n",
      "0.6773255813953488\n",
      "0.6389534883720931\n",
      "0.6505813953488373\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.5773255813953488\n",
      "0.563953488372093\n",
      "0.5406976744186047\n",
      "0.5406976744186047\n",
      "0.5773255813953488\n",
      "0.5656976744186047\n",
      "0.5906976744186047\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6540697674418605\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "Epoch 4001/5000, Loss: 0.0001\n",
      "0.6424418604651163\n",
      "0.6441860465116279\n",
      "0.6575581395348836\n",
      "0.6058139534883721\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6575581395348836\n",
      "0.6058139534883721\n",
      "0.5790697674418605\n",
      "0.5808139534883722\n",
      "0.6441860465116279\n",
      "0.6343023255813953\n",
      "0.6093023255813954\n",
      "0.6209302325581395\n",
      "0.6459302325581394\n",
      "0.6308139534883721\n",
      "0.5976744186046512\n",
      "0.6040697674418606\n",
      "0.6406976744186047\n",
      "0.555813953488372\n",
      "0.49593023255813956\n",
      "0.5459302325581394\n",
      "0.5825581395348837\n",
      "0.5709302325581396\n",
      "0.5941860465116279\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.5924418604651163\n",
      "0.6040697674418606\n",
      "0.5790697674418605\n",
      "0.5924418604651163\n",
      "0.6174418604651163\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.5941860465116279\n",
      "0.6308139534883721\n",
      "0.6406976744186047\n",
      "0.6075581395348837\n",
      "0.6906976744186047\n",
      "0.6424418604651163\n",
      "0.6058139534883721\n",
      "0.5941860465116279\n",
      "0.5575581395348838\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.6174418604651163\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6540697674418605\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.6209302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5825581395348837\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5941860465116279\n",
      "0.5808139534883722\n",
      "0.5924418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.5691860465116279\n",
      "0.5244186046511627\n",
      "0.5325581395348837\n",
      "0.6523255813953488\n",
      "0.5941860465116279\n",
      "0.6691860465116279\n",
      "0.6656976744186047\n",
      "0.7191860465116279\n",
      "0.6691860465116279\n",
      "0.6575581395348836\n",
      "0.6691860465116279\n",
      "0.6808139534883721\n",
      "0.7058139534883721\n",
      "0.7058139534883721\n",
      "0.6808139534883721\n",
      "0.7058139534883721\n",
      "0.7058139534883721\n",
      "0.7290697674418606\n",
      "0.7406976744186048\n",
      "0.7540697674418604\n",
      "0.7406976744186048\n",
      "0.7058139534883721\n",
      "0.7058139534883721\n",
      "0.7174418604651164\n",
      "0.7058139534883721\n",
      "0.7174418604651164\n",
      "0.7040697674418605\n",
      "0.6924418604651162\n",
      "0.6540697674418605\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6424418604651163\n",
      "0.6540697674418605\n",
      "0.5808139534883722\n",
      "Epoch 4201/5000, Loss: 0.0037\n",
      "0.5941860465116279\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6058139534883721\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6058139534883721\n",
      "0.6674418604651163\n",
      "0.702325581395349\n",
      "0.702325581395349\n",
      "0.6540697674418605\n",
      "0.6040697674418606\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6058139534883721\n",
      "0.5691860465116279\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6308139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.6308139534883721\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.5924418604651163\n",
      "0.5808139534883722\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.5959302325581395\n",
      "0.5924418604651163\n",
      "0.6023255813953488\n",
      "0.4895348837209302\n",
      "0.5709302325581396\n",
      "0.5709302325581396\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.5709302325581396\n",
      "0.5959302325581395\n",
      "0.5691860465116279\n",
      "0.5674418604651162\n",
      "0.5674418604651162\n",
      "0.5691860465116279\n",
      "0.5441860465116279\n",
      "0.5825581395348837\n",
      "0.5941860465116279\n",
      "0.5575581395348838\n",
      "0.5441860465116279\n",
      "0.5441860465116279\n",
      "0.5924418604651163\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.5941860465116279\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.5941860465116279\n",
      "0.5825581395348837\n",
      "0.5941860465116279\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.5709302325581396\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.6191860465116279\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6674418604651163\n",
      "0.6558139534883721\n",
      "0.6308139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.5941860465116279\n",
      "0.5691860465116279\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5941860465116279\n",
      "Epoch 4401/5000, Loss: 0.0001\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.5691860465116279\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5459302325581394\n",
      "0.5575581395348838\n",
      "0.5575581395348838\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.5941860465116279\n",
      "0.6075581395348837\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.6075581395348837\n",
      "0.5825581395348837\n",
      "0.5825581395348837\n",
      "0.6075581395348837\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.5959302325581395\n",
      "0.6540697674418605\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.5941860465116279\n",
      "0.6308139534883721\n",
      "0.5924418604651163\n",
      "0.5593023255813954\n",
      "0.6343023255813953\n",
      "0.6906976744186047\n",
      "0.6738372093023256\n",
      "0.6389534883720931\n",
      "0.6872093023255813\n",
      "0.6156976744186047\n",
      "0.6023255813953488\n",
      "0.6290697674418604\n",
      "0.6406976744186047\n",
      "0.6005813953488373\n",
      "0.6505813953488373\n",
      "0.688953488372093\n",
      "0.6406976744186047\n",
      "0.663953488372093\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.5924418604651163\n",
      "0.5808139534883722\n",
      "0.5924418604651163\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6406976744186047\n",
      "0.6156976744186047\n",
      "0.6156976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6156976744186047\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6156976744186047\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6156976744186047\n",
      "0.6523255813953488\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6523255813953488\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6273255813953489\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6273255813953489\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.6389534883720931\n",
      "0.663953488372093\n",
      "0.663953488372093\n",
      "0.663953488372093\n",
      "0.663953488372093\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "Epoch 4601/5000, Loss: 0.0008\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6040697674418606\n",
      "0.6040697674418606\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.555813953488372\n",
      "0.5674418604651162\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5674418604651162\n",
      "0.5924418604651163\n",
      "0.6040697674418606\n",
      "0.6308139534883721\n",
      "0.6540697674418605\n",
      "0.6773255813953488\n",
      "0.6290697674418604\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.5808139534883722\n",
      "0.6540697674418605\n",
      "0.6773255813953488\n",
      "0.6325581395348837\n",
      "0.5540697674418605\n",
      "0.6441860465116279\n",
      "0.5808139534883722\n",
      "0.4709302325581395\n",
      "0.5058139534883721\n",
      "0.5924418604651163\n",
      "0.6040697674418606\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6058139534883721\n",
      "0.6174418604651163\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6290697674418604\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6523255813953488\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6406976744186047\n",
      "0.6174418604651163\n",
      "0.5674418604651162\n",
      "0.6755813953488372\n",
      "0.5424418604651163\n",
      "0.5906976744186047\n",
      "0.6174418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.6156976744186047\n",
      "0.6023255813953488\n",
      "0.5773255813953488\n",
      "0.6023255813953488\n",
      "0.6023255813953488\n",
      "0.6023255813953488\n",
      "0.5790697674418605\n",
      "0.5790697674418605\n",
      "0.5924418604651163\n",
      "0.5691860465116279\n",
      "0.5808139534883722\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.6058139534883721\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5691860465116279\n",
      "0.5924418604651163\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5924418604651163\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.6058139534883721\n",
      "0.5808139534883722\n",
      "0.5808139534883722\n",
      "0.6058139534883721\n",
      "Epoch 4801/5000, Loss: 0.0004\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6174418604651163\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6058139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6790697674418606\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6674418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6424418604651163\n",
      "0.6540697674418605\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6656976744186047\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6558139534883721\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6540697674418605\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6290697674418604\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6674418604651163\n",
      "0.5941860465116279\n",
      "0.6191860465116279\n",
      "0.6540697674418605\n",
      "0.7058139534883721\n",
      "0.5755813953488372\n",
      "0.6174418604651163\n",
      "0.6389534883720931\n",
      "0.5808139534883722\n",
      "0.6808139534883721\n",
      "0.6058139534883721\n",
      "0.6191860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6441860465116279\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6325581395348837\n",
      "0.6093023255813954\n",
      "0.6325581395348837\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6209302325581395\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6209302325581395\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6093023255813954\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6075581395348837\n",
      "0.6075581395348837\n",
      "0.6308139534883721\n",
      "0.6308139534883721\n",
      "0.6424418604651163\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6308139534883721\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n",
      "0.6191860465116279\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, target):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        if 'behavior' in target:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(hidden_size // 2, 2)  # 2 classes para CrossEntropy\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)  # Para regressão\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output\n",
    "\n",
    "\n",
    "# Parâmetros de entrada\n",
    "input_size = X_train.shape[2]  # Número de características de entrada\n",
    "hidden_size = 64               # Tamanho do estado oculto\n",
    "num_layers = 3                 # Número de camadas LSTM\n",
    "dropout = 0.2                  # Taxa de dropout\n",
    "target = 'behavior_target'     # Tipo de alvo para classificação binária\n",
    "epochs = 5000\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Criação do modelo\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, target=target)\n",
    "print(model)\n",
    "\n",
    "# Definindo o otimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Critério para classificação binária (CrossEntropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Criando o DataLoader corretamente (sem embaralhamento, já que é série temporal)\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)),\n",
    "    batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# Dicionário para armazenar as perdas\n",
    "average_losses = {epoch: [] for epoch in range(epochs)}\n",
    "\n",
    "# Loop de treinamento\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        \n",
    "        if target == 'behavior_target':  # Flatten para CrossEntropyLoss\n",
    "            output = output.view(-1, 2)  # Saída com 2 classes\n",
    "            y_batch = y_batch.view(-1)  # Flatten y_batch\n",
    "\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    average_losses[epoch].append(avg_loss)\n",
    "    if epoch % 200  == 0: \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on test data\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)), batch_size=batch_size, shuffle=False)\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            output = model(X_batch)\n",
    "            if 'behavior' in target:\n",
    "                _, predicted = torch.max(output, 1)  # Get the class with highest probability\n",
    "            else:\n",
    "                predicted = output\n",
    "            predictions.append(predicted.numpy())\n",
    "            true_values.append(y_batch.numpy())\n",
    "            \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_values = np.concatenate(true_values, axis=0)\n",
    "\n",
    "    # Calculate error based on target\n",
    "    if 'price' in target:\n",
    "        error = mean_squared_error(true_values, predictions)\n",
    "    else:\n",
    "        error = roc_auc_score(true_values, predictions)\n",
    "        \n",
    "\n",
    "# Plotting the loss decay graph at the end of training\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(epochs), [np.mean(loss) for loss in average_losses.values()], label='Average Training Loss')\n",
    "plt.title(f'Loss Decay Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 30, 504)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x_teste_tenso = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_teste_tenso =torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_teste_tenso)\n",
    "    _, y_pred_1 = torch.max(y_pred, 1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8100,  1.9923],\n",
       "        [-0.0437,  0.0626],\n",
       "        [ 2.7114, -2.7670],\n",
       "        [ 1.4058, -1.3722],\n",
       "        [ 0.6926, -0.6159],\n",
       "        [ 2.3740, -2.4123],\n",
       "        [-3.0176,  3.2952],\n",
       "        [ 1.8870, -1.8796],\n",
       "        [ 2.8068, -2.8558],\n",
       "        [ 3.0198, -3.0871],\n",
       "        [-1.7802,  1.9820],\n",
       "        [-2.8672,  3.1302],\n",
       "        [ 2.1678, -2.1869],\n",
       "        [ 2.3394, -2.3566],\n",
       "        [ 1.2012, -1.1384],\n",
       "        [ 1.4232, -1.3890],\n",
       "        [ 2.1702, -2.1903],\n",
       "        [ 2.7758, -2.8283],\n",
       "        [ 2.9911, -3.0582],\n",
       "        [ 0.8982, -0.8706],\n",
       "        [-2.8161,  3.0749],\n",
       "        [-2.1767,  2.3899],\n",
       "        [ 2.3357, -2.3694],\n",
       "        [ 1.0525, -1.0127],\n",
       "        [-0.9177,  1.0187],\n",
       "        [-2.2534,  2.4468],\n",
       "        [ 1.0014, -1.0276],\n",
       "        [ 2.3311, -2.3633],\n",
       "        [ 1.7359, -1.7374],\n",
       "        [ 2.7437, -2.7947],\n",
       "        [ 0.1000, -0.0151],\n",
       "        [-2.6626,  2.9161],\n",
       "        [-2.3966,  2.6307],\n",
       "        [ 1.9700, -1.9880],\n",
       "        [-0.9189,  1.0727],\n",
       "        [ 0.1303, -0.0504],\n",
       "        [-1.4689,  1.6255],\n",
       "        [ 0.5711, -0.5391],\n",
       "        [ 0.5234, -0.4754],\n",
       "        [ 0.3533, -0.2899],\n",
       "        [-0.5409,  0.6507],\n",
       "        [ 0.4421, -0.3942],\n",
       "        [ 0.9114, -0.8810],\n",
       "        [ 0.2323, -0.1706],\n",
       "        [ 0.9621, -0.9287],\n",
       "        [ 1.6559, -1.6494],\n",
       "        [ 2.0647, -2.0767],\n",
       "        [ 1.2979, -1.2753],\n",
       "        [-1.0589,  1.2005],\n",
       "        [-1.9560,  2.1599],\n",
       "        [-0.3357,  0.4533],\n",
       "        [ 1.1545, -1.1271],\n",
       "        [-0.1207,  0.2016],\n",
       "        [-0.0416,  0.1011],\n",
       "        [ 0.2683, -0.2245],\n",
       "        [ 0.8722, -0.8481],\n",
       "        [ 0.3647, -0.3085],\n",
       "        [-0.8508,  0.9748],\n",
       "        [-1.0829,  1.2211],\n",
       "        [ 1.2826, -1.2726],\n",
       "        [ 0.5233, -0.4679],\n",
       "        [-0.5010,  0.5930],\n",
       "        [ 1.1463, -1.1390]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8020\\2655161623.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Gerar matriz de confusão\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_teste_tenso\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdisp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdisp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Blues'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "    \n",
    "\n",
    "\n",
    "# Gerar matriz de confusão\n",
    "cm = confusion_matrix(y_teste_tenso, y_pred_1)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "\n",
    "# Mostrar a matriz de confusão\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Gerar relatório de métricas\n",
    "report = classification_report(y_teste_tenso, y_pred_1, target_names=[\"Class 0\", \"Class 1\"])\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
