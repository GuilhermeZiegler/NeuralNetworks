{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTAÇÕES NECESSÁRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages: ['numpy', 'pandas', 'scikit-learn', 'joblib', 'pyarrow', 'fastparquet', 'plotly', 'matplotlib', 'MetaTrader5', 'tabulate', 'optuna', 'torch', 'tqdm', 'shap', 'kaleido']\n",
      "numpy is already installed.\n",
      "pandas is already installed.\n",
      "scikit-learn is already installed.\n",
      "joblib is already installed.\n",
      "pyarrow is already installed.\n",
      "fastparquet is already installed.\n",
      "plotly is already installed.\n",
      "matplotlib is already installed.\n",
      "MetaTrader5 is already installed.\n",
      "tabulate is already installed.\n",
      "optuna is already installed.\n",
      "torch is already installed.\n",
      "tqdm is already installed.\n",
      "shap is already installed.\n",
      "kaleido is already installed.\n",
      "All packages are verified.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../scripts'))\n",
    "\n",
    "from myFunctions import install_packages, save_table \n",
    "install_packages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing required packages\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import kaleido\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL CLASSES DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Using the last output of the sequence\n",
    "        return out\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gru_out, hn = self.gru(x)\n",
    "        out = self.fc(gru_out[:, -1, :])  # Using the last output of the sequence\n",
    "        return out\n",
    "\n",
    "# CNN-LSTM Model\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, conv_filters):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_size, conv_filters, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(conv_filters, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # For Conv1d, we need [batch, channels, seq_len]\n",
    "        x = self.conv1d(x)\n",
    "        x = x.permute(0, 2, 1)  # Returning to [batch, seq_len, channels]\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Using the last output of the sequence\n",
    "        return out\n",
    "\n",
    "# CNN-GRU Model\n",
    "class CNNGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, conv_filters):\n",
    "        super(CNNGRUModel, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_size, conv_filters, kernel_size=3, padding=1)\n",
    "        self.gru = nn.GRU(conv_filters, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # For Conv1d, we need [batch, channels, seq_len]\n",
    "        x = self.conv1d(x)\n",
    "        x = x.permute(0, 2, 1)  # Returning to [batch, seq_len, channels]\n",
    "        gru_out, hn = self.gru(x)\n",
    "        out = self.fc(gru_out[:, -1, :])  # Using the last output of the sequence\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESSING DATA TO ARRAYS FOR WINDOWS SIZE AND LOOK FORWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df, exclude_columns=['date', 'day']):\n",
    "    \"\"\"\n",
    "    Escalona todas as colunas do DataFrame, exceto as especificadas em exclude_columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): O DataFrame contendo os dados.\n",
    "        exclude_columns (list): Lista de colunas que não serão escalonadas.\n",
    "\n",
    "    Returns:\n",
    "        df_scaled (pd.DataFrame): DataFrame com as colunas escalonadas.\n",
    "        scalers (dict): Dicionário contendo os escaladores para cada coluna escalonada.\n",
    "    \"\"\"\n",
    "    scalers = {}\n",
    "    columns_to_scale = [col for col in df.columns if col not in exclude_columns]\n",
    "    \n",
    "    df_scaled = df.copy()\n",
    "    for col in columns_to_scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_scaled[col] = scaler.fit_transform(df[[col]])\n",
    "        scalers[col] = scaler\n",
    "    \n",
    "    return df_scaled, scalers\n",
    "\n",
    "\n",
    "def data_to_array(df, window_size, target, features):\n",
    "    \"\"\"\n",
    "    Prepares X and y with targets shifted for the next day after the window.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe containing the data.\n",
    "        window_size (int): The window size (e.g., 7 days).\n",
    "        target (str): The target column name (e.g., 'close_price_target').\n",
    "        features (list): List of feature column names (e.g., ['open', 'high', 'low', 'close']).\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Input features.\n",
    "        y (np.ndarray): Target values.\n",
    "        y_dates (np.ndarray): Dates associated with the targets.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    y_dates = []\n",
    "\n",
    "    for i in range(len(df) - window_size):\n",
    "        # Access the target column directly by its name\n",
    "        target_value = df.iloc[i + window_size][target]\n",
    "        y.append(target_value)\n",
    "        y_dates.append(df.iloc[i + window_size]['date'])\n",
    "        \n",
    "        # Prepare the features using the provided column names\n",
    "        X.append(df.iloc[i:i + window_size][features].values)\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(y_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLITING DATA TO TRAIN AND TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_data(df, test_size=0.15):\n",
    "    \"\"\"\n",
    "    Segments the data into training and test sets based on the test size percentage.\n",
    "    \"\"\"\n",
    "    # Calculates the size of the test set\n",
    "    test_len = int(len(df) * test_size)\n",
    "    \n",
    "    # Segments the data\n",
    "    train_data = df[:-test_len]  # 85% for training\n",
    "    test_data = df[-test_len:]   # 15% for testing\n",
    "    \n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(trial, X_train, y_train, X_test, y_test, target, window_size, look_forward, model_type, study_name, model_dir):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    epochs = trial.suggest_int('epochs', 10, 20)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 3)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 128)\n",
    "\n",
    "    if model_type in ['CNN-LSTM', 'CNN-GRU']:\n",
    "        conv_filters = trial.suggest_categorical('conv_filters', [32, 64, 128])\n",
    "\n",
    "    # Initialize model based on type\n",
    "    if model_type == 'LSTM':\n",
    "        model = LSTMModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "    elif model_type == 'GRU':\n",
    "        model = GRUModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "    elif model_type == 'CNN-LSTM':\n",
    "        model = CNNLSTMModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, conv_filters=conv_filters)\n",
    "    elif model_type == 'CNN-GRU':\n",
    "        model = CNNGRUModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, conv_filters=conv_filters)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss() if 'price' in target else nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    average_losses = {epoch: [] for epoch in range(epochs)}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            if 'behavior' in target:\n",
    "                output = torch.sigmoid(output)\n",
    "                \n",
    "            output = output.view(-1, 1) \n",
    "            y_batch = y_batch.view(-1, 1)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        average_losses[epoch].append(avg_loss)\n",
    "\n",
    "    # Evaluation on test data\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            output = model(X_batch)\n",
    "            predictions.append(output.numpy())\n",
    "            true_values.append(y_batch.numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_values = np.concatenate(true_values, axis=0)\n",
    "\n",
    "    # Calculate error based on target\n",
    "    if 'price' in target:\n",
    "        error = mean_squared_error(true_values, predictions)\n",
    "    else:\n",
    "        predictions = (predictions > 0.5).astype(int)\n",
    "        print(\"predictions\", predictions)\n",
    "        print(\"True Values\", true_values)\n",
    "        error = accuracy_score(true_values, predictions)\n",
    "\n",
    "    # Save loss decay file\n",
    "    loss_decay_file = os.path.join(model_dir, f\"{study_name}_loss_decay.pkl\")\n",
    "    with open(loss_decay_file, 'wb') as f:\n",
    "        joblib.dump(average_losses, f)\n",
    "\n",
    "    # Plot and save loss decay graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(epochs), [np.mean(loss) for loss in average_losses.values()], label='Average Training Loss')\n",
    "    plt.title(f'Loss Decay Over Epochs for {study_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot as an image\n",
    "    loss_plot_file = os.path.join(model_dir, f\"{study_name}_loss_decay_plot.png\")\n",
    "    plt.savefig(loss_plot_file)\n",
    "    plt.close()\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_models(df, targets, target_type, features, windows, look_forwards, max_samples=100):\n",
    "    \"\"\"\n",
    "    Optimizes models using Optuna for hyperparameter tuning. Now prepares the data within the function.\n",
    "    \"\"\"\n",
    "    study_results = {}\n",
    "    input_dir = os.path.join('..', 'data', 'models', target_type).replace(\"/\", \"\\\\\")  \n",
    "    if not os.path.exists(input_dir):\n",
    "        os.makedirs(input_dir)\n",
    "    \n",
    "    exclude_columns = ['date', 'day']\n",
    "    df_scaled, _ = scale_features(df, exclude_columns)\n",
    "    df_scaled, scalers = scale_features(df)\n",
    "    train_data, test_data = segment_data(df_scaled, test_size=0.15)\n",
    "    \n",
    "    for window_size in windows:\n",
    "        for look_forward in look_forwards:\n",
    "            for target in targets:\n",
    "                X_train, y_train, y_dates = data_to_array(train_data.copy(), \n",
    "                                                          window_size,\n",
    "                                                          target, \n",
    "                                                          features)\n",
    "\n",
    "                X_test, y_test, _ = data_to_array(test_data.copy(), \n",
    "                                                  window_size, \n",
    "                                                  target, \n",
    "                                                  features)\n",
    "\n",
    "                model_names = ['LSTM', 'GRU', 'CNN-LSTM', 'CNN-GRU']\n",
    "                for model_type in tqdm(model_names):\n",
    "                    study_name = f\"{model_type}_window_{window_size}_look_forward_{look_forward}_{target}\"\n",
    "                    model_dir = os.path.join(input_dir, study_name).replace(\"/\", \"\\\\\")\n",
    "                    \n",
    "                    if not os.path.exists(model_dir):\n",
    "                        os.makedirs(model_dir)\n",
    "                    \n",
    "                    if 'price' in target:\n",
    "                        direction = 'minimize'\n",
    "                    else:\n",
    "                        direction = 'maximize'\n",
    "                        \n",
    "                        \n",
    "            \n",
    "                    # Creating the Optuna study\n",
    "                    study = optuna.create_study(direction=direction, study_name=study_name)\n",
    "                    study.optimize(\n",
    "                        lambda trial: train_evaluate_model(\n",
    "                            trial, X_train, y_train, X_test, y_test, target, window_size, look_forward, model_type, study_name, model_dir\n",
    "                        ),\n",
    "                        n_trials=max_samples\n",
    "                    )\n",
    "\n",
    "                    study_file = os.path.join(model_dir, f\"{study_name}_study.pkl\").replace(\"/\", \"\\\\\")\n",
    "                    joblib.dump(study, study_file)\n",
    "                    print(f'{study_name} saved to {study_file}')\n",
    "\n",
    "                    best_params = study.best_params\n",
    "                    best_trial_index = study.best_trial.number \n",
    "                    best_trial_value = study.best_value  \n",
    "\n",
    "                    best_params_dict = {\n",
    "                        'best_params': best_params,\n",
    "                        'best_trial_index': best_trial_index,\n",
    "                        'best_trial_value': best_trial_value\n",
    "                    }\n",
    "                    params_file = os.path.join(model_dir, f\"{study_name}_best_params.pkl\").replace(\"/\", \"\\\\\")\n",
    "                    with open(params_file, \"wb\") as f:\n",
    "                        joblib.dump(best_params_dict, f)\n",
    "\n",
    "                    try:\n",
    "                        fig_optimization = vis.plot_optimization_history(study)\n",
    "                        fig_optimization.write_image(os.path.join(model_dir, f\"{study_name}_optimization_history.png\"))\n",
    "\n",
    "                        fig_importances = vis.plot_param_importances(study)\n",
    "                        fig_importances.write_image(os.path.join(model_dir, f\"{study_name}_param_importances.png\"))\n",
    "\n",
    "                        fig_slice = vis.plot_slice(study)\n",
    "                        fig_slice.write_image(os.path.join(model_dir, f\"{study_name}_slice_plot.png\"))\n",
    "\n",
    "                        print(f\"Images saved to {model_dir}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in plotting images for {study_name}: {e}\")\n",
    "\n",
    "\n",
    "                    print(f\"Saved to {model_dir}:\")\n",
    "                    print(f\"Best hyperparameters for {model_type}, window size: {window_size}, look forward: {look_forward}, target {target}: {best_params}\")\n",
    "                    print(f\"Best trial index: {best_trial_index}, Best trial value: {best_trial_value}\")\n",
    "\n",
    "                    study_results[study_name] = {\n",
    "                        \"study\": study,\n",
    "                        \"best_params\": best_params,\n",
    "                        \"best_trial_index\": best_trial_index,\n",
    "                        \"best_trial_value\": best_trial_value,\n",
    "                        \"directory\": model_dir\n",
    "                    }\n",
    "\n",
    "    return study_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dir = os.path.join('..', 'data', 'processed')\n",
    "\n",
    "features_name = 'daily_features.pkl'\n",
    "input_dir_features = os.path.join('..', 'data', 'features')\n",
    "\n",
    "df_daily = pd.read_parquet(os.path.join(input_dir, 'df_daily.parquet')).replace(\"/\", \"\\\\\")\n",
    "df2 = pd.read_parquet(os.path.join(input_dir, 'df_timestamp.parquet')).replace(\"/\", \"\\\\\")\n",
    "daily_features = joblib.load(os.path.join(input_dir_features, 'daily_features.pkl'))\n",
    "timnestamp_features = joblib.load(os.path.join(input_dir_features, '15min_timestamp_features.pkl'))\n",
    "\n",
    "features = daily_features\n",
    "targets = ['behavior_target']\n",
    "windows = [7]#, 15, 30, 45, 60] \n",
    "look_forwards = [1]  \n",
    "\n",
    "\n",
    "# Call the optimize_models function\n",
    "study_results = optimize_models(df=df_daily,\n",
    "                                targets=targets,\n",
    "                                target_type='daily',\n",
    "                                features=features, \n",
    "                                windows=windows,\n",
    "                                look_forwards=look_forwards, \n",
    "                                max_samples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
